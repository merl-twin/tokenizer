#[macro_use]
extern crate lazy_static;
extern crate unicode_segmentation;
extern crate unicode_categories;
extern crate regex;

use unicode_categories::UnicodeCategories;
use regex::Regex;

use unicode_segmentation::{UnicodeSegmentation,UWordBounds};
use std::str::FromStr;
use std::collections::{VecDeque,BTreeSet};

mod emoji;

pub use emoji::EMOJIMAP;



#[derive(Debug,Clone,Copy,PartialEq,PartialOrd)]
pub enum Number {
    Integer(i64),
    Float(f64),
}

#[derive(Debug,Clone,PartialEq,PartialOrd)]
pub enum Numerical {
    //Date(String),
    //Ip(String),
    DotSeparated(String),
    Measures(String),
    //Countable(String),
    Alphanumeric(String),
}

#[derive(Debug,Clone,Copy,Eq,PartialEq,Ord,PartialOrd)]
pub enum Separator {
    Space,
    Tab,
    Newline,
    Unknown,
    Char(char),
}

#[derive(Debug,Clone,Copy,Eq,PartialEq,Ord,PartialOrd)]
pub enum Formater {
    Char(char),
    Joiner, // u{200d}
    Unknown,
}

#[derive(Debug,Clone,PartialEq,PartialOrd,Eq)]
pub enum BasicToken<'t> {
    Alphanumeric(&'t str),
    Number(&'t str),
    Punctuation(&'t str),
    Separator(&'t str),
    Formater(&'t str),
    Mixed(&'t str),
}
impl<'t> BasicToken<'t> {
    fn len(&self) -> usize {
        match &self {
            BasicToken::Alphanumeric(s) |
            BasicToken::Number(s) |
            BasicToken::Punctuation(s) |
            BasicToken::Mixed(s) |
            BasicToken::Formater(s) |
            BasicToken::Separator(s) => s.len(),
        }
    }
}

#[derive(Debug,Clone,PartialEq,PartialOrd)]
pub enum Token {
    Word(String),
    StrangeWord(String),
    Numerical(Numerical),
    Hashtag(String),
    Mention(String),
    Punctuation(String),
    Number(Number),
    Emoji(String),
    Unicode(String),
    Separator(Separator),
    UnicodeFormater(Formater),
    UnicodeModifier(char),
    Url(String),
    BBCode { left: Vec<PositionalToken>, right: Vec<PositionalToken> },
}

#[derive(Debug,Clone,PartialEq,PartialOrd)]
pub struct PositionalToken {
    pub offset: usize,
    pub length: usize,
    pub token: Token,   
}

#[derive(Debug,Copy,Clone,PartialEq,Eq,PartialOrd,Ord)]
pub enum TokenizerOptions {
    DetectHtml,
    DetectBBCode,
}

struct ExtWordBounds<'t> {
    offset: usize,
    initial: &'t str,
    bounds: UWordBounds<'t>,
    buffer: VecDeque<&'t str>,
    exceptions: BTreeSet<char>,
}
impl<'t> ExtWordBounds<'t> {
    fn new<'a>(s: &'a str) -> ExtWordBounds<'a> {
        ExtWordBounds {
            offset: 0,
            initial: s,
            bounds: s.split_word_bounds(),
            buffer: VecDeque::new(),
            exceptions: ['\u{200d}'].iter().cloned().collect(),
        }
    }
}
impl<'t> Iterator for ExtWordBounds<'t> {
    type Item = &'t str;
    fn next(&mut self) -> Option<Self::Item> {
        if self.buffer.len() > 0 { return self.buffer.pop_front(); }
        match self.bounds.next() {
            None => None,
            Some(w) => {
                let mut len = 0;
                let mut chs = w.chars().peekable();
                while let Some(c) = chs.next() {
                    if c.is_other_format() {
                        if (!self.exceptions.contains(&c))||
                            ((c == '\u{200d}') && chs.peek().is_none()) {
                            if len > 0 {
                                self.buffer.push_back(&self.initial[self.offset .. self.offset+len]);
                                self.offset += len;
                                len = 0;
                            }
                            self.buffer.push_back(&self.initial[self.offset .. self.offset+c.len_utf8()]);
                            self.offset += c.len_utf8();
                        } else {
                            len += c.len_utf8();
                        }
                    } else {
                        len += c.len_utf8();
                    }
                }
                if len > 0 {
                    self.buffer.push_back(&self.initial[self.offset .. self.offset+len]);
                    self.offset += len;
                }
                self.next()
            },
        }
    }
}

fn one_char_word(w: &str) -> Option<char> {
    // returns Some(char) if len in char == 1, None otherwise
    let mut cs = w.chars();
    match (cs.next(),cs.next()) {
        (Some(c),None) => Some(c),
        _ => None,
    }
}

pub struct Breaker<'t> {
    offset: usize,
    initial: &'t str,
    bounds: std::iter::Peekable<ExtWordBounds<'t>>,
}
impl<'t> Breaker<'t> {
    pub fn new<'a>(s: &'a str) -> Breaker<'a> {
        Breaker {
            offset: 0,
            initial: s,
            bounds: ExtWordBounds::new(s).peekable(),
        }
    }
}
impl<'t> Iterator for Breaker<'t> {
    type Item = BasicToken<'t>;
    fn next(&mut self) -> Option<Self::Item> {
        match self.bounds.next() {
            Some(w) => {
                if let Some(c) = one_char_word(w) {
                    if c.is_ascii_punctuation() || c.is_punctuation() || c.is_whitespace() || c.is_other_format() {
                        let mut len = c.len_utf8();
                        loop {
                            match self.bounds.peek() {
                                Some(p) if *p==w => len += c.len_utf8(),
                                _ => break,
                            }
                            self.bounds.next();
                        }
                        let p = &self.initial[self.offset .. self.offset+len];
                        self.offset += len;
                        if c.is_ascii_punctuation() || c.is_punctuation() {
                            return Some(BasicToken::Punctuation(p));
                        }
                        if c.is_other_format() {
                            return Some(BasicToken::Formater(p));
                        } else {
                            return Some(BasicToken::Separator(p));
                        }
                    }
                }
                let mut an = true;
                let mut num = true;
                let mut dot_count = 0;
                for c in w.chars() {
                    an = an && (c.is_alphanumeric() || (c == '.') || (c == '\'') || (c == '-') || (c == '+') || (c == '_'));
                    num = num && (c.is_digit(10) || (c == '.') || (c == '-') || (c == '+'));
                    if c == '.' { dot_count += 1; }
                }
                if dot_count>1 { num = false; }
                self.offset += w.len();
                if num {
                    return Some(BasicToken::Number(w));
                }
                if an {
                    return Some(BasicToken::Alphanumeric(w));
                }
                Some(BasicToken::Mixed(w))
            },
            None => None,
        }
    }
}

pub trait Tokenizer {
    fn next_token(&mut self) -> Option<PositionalToken>;
}

fn detect_bbcodes(s: &str) -> VecDeque<(usize,usize,usize)> {
    lazy_static! {
        static ref RE: Regex = Regex::new(r"\[(.*?)\|(.*?)\]").unwrap();
    }
    let mut res = VecDeque::new(); 
    for cap in RE.captures_iter(s) {
        //println!("{:?} {:?}",cap,cap.get(0).map(|m0| (m0.start(),m0.end()-m0.start())));
        match (cap.get(0),cap.get(1),cap.get(2)) {
            (Some(m0),Some(m1),Some(m2)) => res.push_back((m0.start(),m1.end()-m1.start(),m2.end()-m2.start())),
            _ => continue,
        }
    }
    res
}

fn detect_html(s: &str) -> usize {
    lazy_static! {
        static ref RE: Regex = Regex::new(r"</?\w+?.*?>").unwrap();
    }
    let mut res = VecDeque::new(); 
    for cap in RE.captures_iter(s) {
        //println!("{:?} {:?}",cap,cap.get(0).map(|m0| (m0.start(),m0.end()-m0.start())));
        match cap.get(0) {
            Some(m0) => res.push_back((m0.start(),m0.end()-m0.start())),
            _ => continue,
        }
    }
    res.len()
}

#[derive(Debug)]
pub enum Untokenizable {
    Html,
}

pub struct Tokens<'t> {
    offset: usize,
    bounds: Breaker<'t>,
    buffer: VecDeque<BasicToken<'t>>,
    bbcodes: VecDeque<(usize,usize,usize)>,
}
impl<'t> Tokens<'t> {
    fn new<'a>(s: &'a str, options: BTreeSet<TokenizerOptions>) -> Result<Tokens<'a>,Untokenizable> {
        if options.contains(&TokenizerOptions::DetectHtml)&&(detect_html(s)>5) {
            return Err(Untokenizable::Html)
        }
        Ok(Tokens {
            offset: 0,
            bounds: Breaker::new(s),
            buffer: VecDeque::new(),
            bbcodes: if options.contains(&TokenizerOptions::DetectBBCode) { detect_bbcodes(s) } else { VecDeque::new() },
        })
    }
    fn basic<'a>(s: &'a str) -> Tokens<'a> {
        Tokens {
            offset: 0,
            bounds: Breaker::new(s),
            buffer: VecDeque::new(),
            bbcodes: VecDeque::new(),
        }
    }
    fn basic_separator_to_pt(&mut self, s: &str) -> PositionalToken {
        let tok = PositionalToken {
            offset: self.offset,
            length: s.len(),
            token: Token::Separator(match s.chars().next() {
                Some(' ') => Separator::Space,
                Some('\n') => Separator::Newline,
                Some('\t') => Separator::Tab,
                Some(c) => Separator::Char(c),
                None => Separator::Unknown,
            })
        };
        self.offset += s.len();
        tok
    }
    fn basic_formater_to_pt(&mut self, s: &str) -> PositionalToken {
        let tok = PositionalToken {
            offset: self.offset,
            length: s.len(),
            token: Token::UnicodeFormater(match s.chars().next() {
                Some('\u{200d}') => Formater::Joiner,
                Some(c) => Formater::Char(c),
                None => Formater::Unknown,
            }),
        };
        self.offset += s.len();
        tok
    }   
    fn basic_number_to_pt(&mut self, s: &str) -> PositionalToken {
        let tok = PositionalToken {
            offset: self.offset,
            length: s.len(),
            token: match i64::from_str(s) {
                Ok(n) => Token::Number(Number::Integer(n)),
                Err(_) => {
                    match f64::from_str(s) {
                        Ok(n) => Token::Number(Number::Float(n)),
                        Err(..) => Token::Word(s.to_string()),
                    }
                }
            },
        };
        self.offset += s.len();
        tok
    }
    fn basic_mixed_to_pt(&mut self, s: &str) -> PositionalToken {
        let tok = PositionalToken {
            offset: self.offset,
            length: s.len(),
            token: {
                let rs = s.replace("\u{fe0f}","");
                match EMOJIMAP.get(&rs as &str) {
                    Some(em) => Token::Emoji(em.to_string()),
                    None => match one_char_word(&rs) {
                        Some(c) if c.is_symbol_modifier() => Token::UnicodeModifier(c),
                        Some(_) | None => Token::Unicode({
                            let mut us = "".to_string();
                            for c in rs.chars() {
                                if us!="" { us += "_"; }
                                us += "u";
                                let ns = format!("{}",c.escape_unicode());
                                us += &ns[3 .. ns.len()-1];
                            }
                            us
                        })
                    },
                }
            }
        };
        self.offset += s.len();
        tok
    }
    fn basic_alphanumeric_to_pt(&mut self, s: &str) -> PositionalToken {
        /*
        Word
        StrangeWord
        pub enum Numerical {
            Date(String),
            Ip(String),
            DotSeparated(String),
            Countable(String),
            Measures(String),
            Alphanumeric(String),
        }*/
        //let mut wrd = true;
        let mut digits = false;
        let mut digits_begin_only = false;
        let mut dots = false;
        let mut alphas_and_apos = false;
        let mut other = false;

        let mut start_digit = true;
        for c in s.chars() {
            if start_digit && (!c.is_digit(10)) { start_digit = false; }
            match c {
                c @ _ if c.is_digit(10) => {
                    digits = true;
                    if start_digit { digits_begin_only = true; }
                    else { digits_begin_only = false; }
                },
                c @ _ if c.is_alphabetic() => { alphas_and_apos = true; },
                '\'' => { alphas_and_apos = true; },
                '.' => { dots = true; },
                _ => { other = true; },
            }
        }
        let tok = PositionalToken {
            offset: self.offset,
            length: s.len(),
            token: match (digits,digits_begin_only,dots,alphas_and_apos,other) {
                (true,false,true,false,false) => {
                    // TODO: Date, Ip, DotSeparated
                    Token::Numerical(Numerical::DotSeparated(s.to_string()))
                },
                (true,true,_,true,false) => {
                    // TODO: Countable or Measures
                    Token::Numerical(Numerical::Measures(s.to_string()))
                },
                (true, _, _, _, _) => {
                    // Numerical trash, ids, etc.
                    Token::Numerical(Numerical::Alphanumeric(s.to_string()))
                }
                (false,false,_,true,false) => {
                    // Word
                    Token::Word(s.to_string())
                },
                (false,false,_,_,_) => {
                    // Strange
                    Token::StrangeWord(s.to_string())
                },
                (false,true,_,_,_) => unreachable!(),
            },
        };
        self.offset += s.len();
        tok
    }
    fn basic_punctuation_to_pt(&mut self, s: &str) -> PositionalToken {
        let tok = PositionalToken {
            offset: self.offset,
            length: s.len(),
            token: Token::Punctuation(s.to_string()),
        };
        self.offset += s.len();
        tok
    }
    fn check_url(&mut self) -> Option<PositionalToken> {
        let check = if self.buffer.len()>3 {
            match (&self.buffer[0],&self.buffer[1],&self.buffer[2]) {
                (BasicToken::Alphanumeric("http"),BasicToken::Punctuation(":"),BasicToken::Punctuation("//")) |
                (BasicToken::Alphanumeric("https"),BasicToken::Punctuation(":"),BasicToken::Punctuation("//")) => true,
                _ => false,
            }
        } else { false };
        if check {
            let mut url = "".to_string();
            let tag_bound = {
                if self.bbcodes.len()>0 { Some(self.bbcodes[0].0) } else { None }
            };
            loop {
                if let Some(b) = tag_bound {
                    if (self.offset + url.len()) >= b { break; }
                }
                match self.buffer.pop_front() {
                    None => break,
                    Some(BasicToken::Separator(s)) => {
                        self.buffer.push_front(BasicToken::Separator(s));
                        break;
                    },
                    Some(BasicToken::Alphanumeric(s)) |
                    Some(BasicToken::Number(s)) |
                    Some(BasicToken::Punctuation(s)) |
                    Some(BasicToken::Formater(s)) |
                    Some(BasicToken::Mixed(s)) => {
                        url += s;
                    },
                }
            }
            let len = url.len();
            let tok = PositionalToken {
                offset: self.offset,
                length: len,
                token: Token::Url(url),
            };
            self.offset += len;
            Some(tok)
        } else { None }
    }
    fn check_hashtag(&mut self) -> Option<PositionalToken> {
        let tok = if self.buffer.len()>1 {
            match (&self.buffer[0],&self.buffer[1]) {
                (BasicToken::Punctuation("#"),BasicToken::Alphanumeric(s)) |
                (BasicToken::Punctuation("#"),BasicToken::Number(s)) => {
                    let tok = PositionalToken {
                        offset: self.offset,
                        length: s.len()+1,
                        token: Token::Hashtag(format!("{}",s)),
                    };
                    self.offset += s.len()+1;
                    Some(tok)
                },
                _ => None,
            }
        } else { None };
        if tok.is_some() {
            self.buffer.pop_front();
            self.buffer.pop_front();
        }
        tok
    }
    fn check_mention(&mut self) -> Option<PositionalToken> {
        let tok = if self.buffer.len()>1 {
            match (&self.buffer[0],&self.buffer[1]) {
                (BasicToken::Punctuation("@"),BasicToken::Alphanumeric(s)) |
                (BasicToken::Punctuation("@"),BasicToken::Number(s)) => {
                    let tok = PositionalToken {
                        offset: self.offset,
                        length: s.len()+1,
                        token: Token::Mention(format!("{}",s)),
                    };
                    self.offset += s.len()+1;
                    Some(tok)
                },
                _ => None,
            }
        } else { None };
        if tok.is_some() {
            self.buffer.pop_front();
            self.buffer.pop_front();
        }
        tok
    }
    fn check_bb_code(&mut self, text_len: usize, data_len: usize) -> Option<PositionalToken> {
        if self.buffer.len() >= (text_len+data_len+3) {
            if (self.buffer[0] == BasicToken::Punctuation("["))&&
                (self.buffer[text_len+1] == BasicToken::Punctuation("|"))&&
                (self.buffer[text_len+data_len+2] == BasicToken::Punctuation("]")) {
                    let offset = self.offset;
                    self.buffer.pop_front(); self.offset += 1;
                    let mut tail = self.buffer.split_off(text_len);
                    let mut text_vec = Vec::new(); 
                    while let Some(t) = self.next_from_buffer() {
                        text_vec.push(t);
                    }
                    std::mem::swap(&mut tail,&mut self.buffer);
                    self.buffer.pop_front(); self.offset += 1;
                    tail = self.buffer.split_off(data_len);
                    let mut data_vec = Vec::new(); 
                    while let Some(t) = self.next_from_buffer() {
                        data_vec.push(t);
                    }
                    std::mem::swap(&mut tail,&mut self.buffer);
                    self.buffer.pop_front(); self.offset += 1;
                    Some(PositionalToken {
                        offset: offset,
                        length: self.offset - offset,
                        token: Token::BBCode{ left: text_vec, right: data_vec },
                    })
                } else { None }
        } else { None }

    }
    fn next_from_buffer(&mut self) -> Option<PositionalToken> {
        if let Some(t) = self.check_url() { return Some(t); }
        if let Some(t) = self.check_hashtag() { return Some(t); }
        if let Some(t) = self.check_mention() { return Some(t); }
        match self.buffer.pop_front() {
            Some(BasicToken::Alphanumeric(s)) => Some(self.basic_alphanumeric_to_pt(s)),
            Some(BasicToken::Number(s)) => Some(self.basic_number_to_pt(s)),
            Some(BasicToken::Punctuation(s)) => Some(self.basic_punctuation_to_pt(s)),
            Some(BasicToken::Mixed(s)) => Some(self.basic_mixed_to_pt(s)),
            Some(BasicToken::Separator(s)) => Some(self.basic_separator_to_pt(s)),
            Some(BasicToken::Formater(s)) => Some(self.basic_formater_to_pt(s)),
            None => None,
        }
    }
}

impl<'t> Tokenizer for Tokens<'t> {
    fn next_token(&mut self) -> Option<PositionalToken> {
        loop {
            if self.buffer.len()>0 {
                if (self.bbcodes.len()>0)&&(self.bbcodes[0].0 == self.offset) {
                    let get_len = self.bbcodes[0].1 + self.bbcodes[0].2 + 3;
                    let (text_from,text_len) = (self.bbcodes[0].0+1,self.bbcodes[0].1);
                    let (text2_from,text2_len) = (self.bbcodes[0].0+self.bbcodes[0].1+2,self.bbcodes[0].2);
                    let mut cur_len = 0;
                    let mut cur_off = self.offset;
                    let mut buf1_len = 0;
                    let mut buf2_len = 0;
                    for bt in &self.buffer {
                        if (cur_off>=text_from)&&(cur_off<(text_from+text_len)) { buf1_len += 1; } 
                        if (cur_off>=text2_from)&&(cur_off<(text2_from+text2_len)) { buf2_len += 1; }
                        cur_off += bt.len();
                        cur_len += bt.len();
                    }
                    while cur_len<get_len {
                        match self.bounds.next() {
                            None => break,
                            Some(bt) => {
                                if (cur_off>=text_from)&&(cur_off<(text_from+text_len)) { buf1_len += 1; } 
                                if (cur_off>=text2_from)&&(cur_off<(text2_from+text2_len)) { buf2_len += 1; }
                                cur_off += bt.len();
                                cur_len += bt.len();
                                self.buffer.push_back(bt);
                            }
                        }
                    }
                    //println!("{:?} {} {} {}",self.bbcodes[0],self.buffer.len(),buf1_len,buf2_len);
                    //println!("{:?}",self.buffer);
                    self.bbcodes.pop_front();
                    if let Some(t) = self.check_bb_code(buf1_len,buf2_len) { return Some(t); }
                }
                return self.next_from_buffer();
            } else {
                loop {
                    match self.bounds.next() {
                        Some(BasicToken::Separator(s)) => {
                            self.buffer.push_back(BasicToken::Separator(s));
                            return self.next_token();
                        },
                        Some(bt) => self.buffer.push_back(bt),
                        None if self.buffer.len()>0 => return self.next_token(),
                        None => return None,
                    }
                }
            }
        }
    }
}

impl<'t> Iterator for Tokens<'t> {
    type Item = PositionalToken;

    fn next(&mut self) -> Option<Self::Item> {
        self.next_token()
    }
}

pub trait IntoTokenizer {
    type IntoTokens: Tokenizer;
    fn into_tokens(self) -> Result<Self::IntoTokens,Untokenizable>;
    fn into_tokens_with_options(self, options:BTreeSet<TokenizerOptions>) -> Result<Self::IntoTokens,Untokenizable>;
    fn basic_tokens(self) -> Self::IntoTokens;
}
impl<'t> IntoTokenizer for &'t str {
    type IntoTokens = Tokens<'t>;
    fn into_tokens(self) -> Result<Self::IntoTokens,Untokenizable> {
        Tokens::new(self,vec![TokenizerOptions::DetectBBCode,TokenizerOptions::DetectHtml].into_iter().collect())
    }
    fn into_tokens_with_options(self, options:BTreeSet<TokenizerOptions>) -> Result<Self::IntoTokens,Untokenizable> {
        Tokens::new(self,options)
    }
    fn basic_tokens(self) -> Self::IntoTokens {
        Tokens::basic(self)
    }
}



#[cfg(test)]
mod test {
    use super::*;

    fn print_pt(tok: &PositionalToken) -> String {
        let mut r = match &tok.token {
            Token::BBCode{ left, right } => {
                let left = print_pts(left);
                let right = print_pts(right);
                format!("PositionalToken {{ offset: {}, length: {}, token: Token::BBCode {{ left: vec![\n{}], right: vec![\n{}] }} }},",tok.offset,tok.length,left,right)
            },
            _ => format!("PositionalToken {{ offset: {}, length: {}, token: Token::{:?} }},",tok.offset,tok.length,tok.token),
        };
        r = r.replace("\")","\".to_string())");
        r
    }

    fn print_pts(lib_res: &Vec<PositionalToken>) -> String {
        let mut r = String::new();
        for tok in lib_res {        
            r += &print_pt(&tok);
            r += "\n";
        }
        r
    }

    fn print_result(lib_res: &Vec<PositionalToken>) {
        let mut r = print_pts(lib_res);
        r = r.replace("Separator(","Separator(Separator::");
        r = r.replace("UnicodeFormater(","UnicodeFormater(Formater::");
        r = r.replace("Number(","Number(Number::");
        r = r.replace("Numerical(","Numerical(Numerical::");
        println!("{}",r);
    }

    fn check_results(result: &Vec<PositionalToken>, lib_res: &Vec<PositionalToken>, _uws: &str) {
        assert_eq!(result.len(),lib_res.len());
        for i in 0 .. result.len() {
            assert_eq!(result[i],lib_res[i]);
        }
    }
    
    #[test]
    fn general() {
        let uws = "The quick (\"brown\") fox can't jump 32.3 feet, right? 4pda etc. qeq U.S.A  asd\n\n\nBrr, it's 29.3¬∞F!\n –†—É—Å—Å–∫–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ #36.6 –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –¥–µ–ª–µ–Ω–∏—è –ø–æ —é–Ω–∏–∫–æ–¥-—Å–ª–æ–≤–∞–º...\n";
        let result = vec![
            PositionalToken { offset: 0, length: 3, token: Token::Word("The".to_string()) },
            PositionalToken { offset: 3, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 4, length: 5, token: Token::Word("quick".to_string()) },
            PositionalToken { offset: 9, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 10, length: 1, token: Token::Punctuation("(".to_string()) },
            PositionalToken { offset: 11, length: 1, token: Token::Punctuation("\"".to_string()) },
            PositionalToken { offset: 12, length: 5, token: Token::Word("brown".to_string()) },
            PositionalToken { offset: 17, length: 1, token: Token::Punctuation("\"".to_string()) },
            PositionalToken { offset: 18, length: 1, token: Token::Punctuation(")".to_string()) },
            PositionalToken { offset: 19, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 20, length: 3, token: Token::Word("fox".to_string()) },
            PositionalToken { offset: 23, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 24, length: 5, token: Token::Word("can\'t".to_string()) },
            PositionalToken { offset: 29, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 30, length: 4, token: Token::Word("jump".to_string()) },
            PositionalToken { offset: 34, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 35, length: 4, token: Token::Number(Number::Float(32.3)) },
            PositionalToken { offset: 39, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 40, length: 4, token: Token::Word("feet".to_string()) },
            PositionalToken { offset: 44, length: 1, token: Token::Punctuation(",".to_string()) },
            PositionalToken { offset: 45, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 46, length: 5, token: Token::Word("right".to_string()) },
            PositionalToken { offset: 51, length: 1, token: Token::Punctuation("?".to_string()) },
            PositionalToken { offset: 52, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 53, length: 4, token: Token::Numerical(Numerical::Measures("4pda".to_string())) }, // TODO
            PositionalToken { offset: 57, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 58, length: 3, token: Token::Word("etc".to_string()) },
            PositionalToken { offset: 61, length: 1, token: Token::Punctuation(".".to_string()) },
            PositionalToken { offset: 62, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 63, length: 3, token: Token::Word("qeq".to_string()) },
            PositionalToken { offset: 66, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 67, length: 5, token: Token::Word("U.S.A".to_string()) },
            PositionalToken { offset: 72, length: 2, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 74, length: 3, token: Token::Word("asd".to_string()) },
            PositionalToken { offset: 77, length: 3, token: Token::Separator(Separator::Newline) },
            PositionalToken { offset: 80, length: 3, token: Token::Word("Brr".to_string()) },
            PositionalToken { offset: 83, length: 1, token: Token::Punctuation(",".to_string()) },
            PositionalToken { offset: 84, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 85, length: 4, token: Token::Word("it\'s".to_string()) },
            PositionalToken { offset: 89, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 90, length: 4, token: Token::Number(Number::Float(29.3)) },
            PositionalToken { offset: 94, length: 2, token: Token::Unicode("ub0".to_string()) },
            PositionalToken { offset: 96, length: 1, token: Token::Word("F".to_string()) },
            PositionalToken { offset: 97, length: 1, token: Token::Punctuation("!".to_string()) },
            PositionalToken { offset: 98, length: 1, token: Token::Separator(Separator::Newline) },
            PositionalToken { offset: 99, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 100, length: 14, token: Token::Word("–†—É—Å—Å–∫–æ–µ".to_string()) },
            PositionalToken { offset: 114, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 115, length: 22, token: Token::Word("–ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ".to_string()) },
            PositionalToken { offset: 137, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 138, length: 5, token: Token::Hashtag("36.6".to_string()) },
            PositionalToken { offset: 143, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 144, length: 6, token: Token::Word("–¥–ª—è".to_string()) },
            PositionalToken { offset: 150, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 151, length: 24, token: Token::Word("—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è".to_string()) },
            PositionalToken { offset: 175, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 176, length: 14, token: Token::Word("–¥–µ–ª–µ–Ω–∏—è".to_string()) },
            PositionalToken { offset: 190, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 191, length: 4, token: Token::Word("–ø–æ".to_string()) },
            PositionalToken { offset: 195, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 196, length: 12, token: Token::Word("—é–Ω–∏–∫–æ–¥".to_string()) },
            PositionalToken { offset: 208, length: 1, token: Token::Punctuation("-".to_string()) },
            PositionalToken { offset: 209, length: 12, token: Token::Word("—Å–ª–æ–≤–∞–º".to_string()) },
            PositionalToken { offset: 221, length: 3, token: Token::Punctuation("...".to_string()) },
            PositionalToken { offset: 224, length: 1, token: Token::Separator(Separator::Newline) },
            ];
        let lib_res = uws.into_tokens().unwrap().collect::<Vec<_>>();
        check_results(&result,&lib_res,uws);
    }

    #[test]
    #[ignore]
    fn woman_bouncing_ball() {
        let uws = "\u{26f9}\u{200d}\u{2640}";
        let result = vec![PositionalToken { offset: 0, length: 9, token: Token::Emoji("woman_bouncing_ball".to_string()) }];
        let lib_res = uws.into_tokens().unwrap().collect::<Vec<_>>();
        check_results(&result,&lib_res,uws);
        //print_result(&lib_res); panic!("")
        panic!();
    } 
    
    #[test]
    fn emoji_and_rusabbr() {
        let uws = "üá∑üá∫ üá∏üáπ\nüë±üèøüë∂üèΩüë®üèΩ\nüë±\n–°.–°.–°.–†.\nüë®‚Äçüë©‚Äçüë¶‚Äçüë¶\nüß†\n";
        let result = vec![
            PositionalToken { offset: 0, length: 8, token: Token::Emoji("russia".to_string()) },
            PositionalToken { offset: 8, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 9, length: 8, token: Token::Emoji("sao_tome_and_principe".to_string()) },
            PositionalToken { offset: 17, length: 1, token: Token::Separator(Separator::Newline) },
            PositionalToken { offset: 18, length: 8, token: Token::Emoji("blond_haired_person_dark_skin_tone".to_string()) },
            PositionalToken { offset: 26, length: 8, token: Token::Emoji("baby_medium_skin_tone".to_string()) },
            PositionalToken { offset: 34, length: 8, token: Token::Emoji("man_medium_skin_tone".to_string()) },
            PositionalToken { offset: 42, length: 1, token: Token::Separator(Separator::Newline) },
            PositionalToken { offset: 43, length: 4, token: Token::Emoji("blond_haired_person".to_string()) },
            PositionalToken { offset: 47, length: 1, token: Token::Separator(Separator::Newline) },
            PositionalToken { offset: 48, length: 11, token: Token::Word("–°.–°.–°.–†".to_string()) },
            PositionalToken { offset: 59, length: 1, token: Token::Punctuation(".".to_string()) },
            PositionalToken { offset: 60, length: 1, token: Token::Separator(Separator::Newline) },
            PositionalToken { offset: 61, length: 25, token: Token::Emoji("family_man_woman_boy_boy".to_string()) },
            PositionalToken { offset: 86, length: 1, token: Token::Separator(Separator::Newline) },
            PositionalToken { offset: 87, length: 4, token: Token::Emoji("brain".to_string()) },
            PositionalToken { offset: 91, length: 1, token: Token::Separator(Separator::Newline) },
            ];
        
        let lib_res = uws.into_tokens().unwrap().collect::<Vec<_>>();
        check_results(&result,&lib_res,uws);
        //print_result(&lib_res); panic!();
    }

    #[test]
    fn hashtags_mentions_urls() {
        let uws = "\nSome ##text with #hashtags and @other components\nadfa wdsfdf asdf asd http://asdfasdfsd.com/fasdfd/sadfsadf/sdfas/12312_12414/asdf?fascvx=fsfwer&dsdfasdf=fasdf#fasdf asdfa sdfa sdf\nasdfas df asd who@bla-bla.com asdfas df asdfsd\n";
        let result = vec![
            PositionalToken { offset: 0, length: 1, token: Token::Separator(Separator::Newline) },
            PositionalToken { offset: 1, length: 4, token: Token::Word("Some".to_string()) },
            PositionalToken { offset: 5, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 6, length: 2, token: Token::Punctuation("##".to_string()) },
            PositionalToken { offset: 8, length: 4, token: Token::Word("text".to_string()) },
            PositionalToken { offset: 12, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 13, length: 4, token: Token::Word("with".to_string()) },
            PositionalToken { offset: 17, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 18, length: 9, token: Token::Hashtag("hashtags".to_string()) },
            PositionalToken { offset: 27, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 28, length: 3, token: Token::Word("and".to_string()) },
            PositionalToken { offset: 31, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 32, length: 6, token: Token::Mention("other".to_string()) },
            PositionalToken { offset: 38, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 39, length: 10, token: Token::Word("components".to_string()) },
            PositionalToken { offset: 49, length: 1, token: Token::Separator(Separator::Newline) },
            PositionalToken { offset: 50, length: 4, token: Token::Word("adfa".to_string()) },
            PositionalToken { offset: 54, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 55, length: 6, token: Token::Word("wdsfdf".to_string()) },
            PositionalToken { offset: 61, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 62, length: 4, token: Token::Word("asdf".to_string()) },
            PositionalToken { offset: 66, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 67, length: 3, token: Token::Word("asd".to_string()) },
            PositionalToken { offset: 70, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 71, length: 95, token: Token::Url("http://asdfasdfsd.com/fasdfd/sadfsadf/sdfas/12312_12414/asdf?fascvx=fsfwer&dsdfasdf=fasdf#fasdf".to_string()) },
            PositionalToken { offset: 166, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 167, length: 5, token: Token::Word("asdfa".to_string()) },
            PositionalToken { offset: 172, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 173, length: 4, token: Token::Word("sdfa".to_string()) },
            PositionalToken { offset: 177, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 178, length: 3, token: Token::Word("sdf".to_string()) },
            PositionalToken { offset: 181, length: 1, token: Token::Separator(Separator::Newline) },
            PositionalToken { offset: 182, length: 6, token: Token::Word("asdfas".to_string()) },
            PositionalToken { offset: 188, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 189, length: 2, token: Token::Word("df".to_string()) },
            PositionalToken { offset: 191, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 192, length: 3, token: Token::Word("asd".to_string()) },
            PositionalToken { offset: 195, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 196, length: 3, token: Token::Word("who".to_string()) },
            PositionalToken { offset: 199, length: 4, token: Token::Mention("bla".to_string()) },
            PositionalToken { offset: 203, length: 1, token: Token::Punctuation("-".to_string()) },
            PositionalToken { offset: 204, length: 7, token: Token::Word("bla.com".to_string()) },
            PositionalToken { offset: 211, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 212, length: 6, token: Token::Word("asdfas".to_string()) },
            PositionalToken { offset: 218, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 219, length: 2, token: Token::Word("df".to_string()) },
            PositionalToken { offset: 221, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 222, length: 6, token: Token::Word("asdfsd".to_string()) },
            PositionalToken { offset: 228, length: 1, token: Token::Separator(Separator::Newline) },
            ];
        let lib_res = uws.into_tokens().unwrap().collect::<Vec<_>>();
        check_results(&result,&lib_res,uws);
        //print_result(&lib_res); panic!("")
    }

    #[test]
    fn bb_code() {
        let uws = "[Oxana Putan|1712640565] shared a [post|100001150683379_1873048549410150]. \nAndrew\n[link|https://www.facebook.com/100001150683379/posts/1873048549410150]\n–î—Ä—É–∑—å—è –º–æ–∏, –∏–∑–¥–∞—Ç–µ–ª–∏, —Ä–µ–¥–∞–∫—Ç–æ—Ä—ã, –ø—Ä–æ—Å–≤–µ—Ç–∏—Ç–µ–ª–∏, –∫—É–ª—å—Ç—É—Ä—Ç—Ä–µ–≥–µ—Ä—ã, —Å—É–±—ä–µ–∫—Ç—ã –º–∏—Ä–æ–≤–æ–≥–æ —Ä—ã–Ω–∫–∞ –∏ —Ç—É —Ö—É–º –∏—Ç –µ—â—ë –º–µ–π –∫–æ–Ω—Å—ë—Ä–Ω.\n–ù–∞ —Ç–µ–∫—É—â–∏–π –º–æ–º–µ–Ω—Ç —è –ª–∏—à–µ–Ω –±—ã–ª–æ–π –ø–æ–¥–≤–∏–∂–Ω–æ—Å—Ç–∏, —Ö–æ—Ç—å –∏ –∫–æ–≤—ã–ª—è—é –ø–æ –±–æ–ª—å–Ω–∏—á–Ω—ã—Ö –∫–æ—Ä–∏–¥–æ—Ä–∞–º –ø–æ —Ä–∞–∑–Ω—ã–º –Ω—É–∂–¥–∞–º –∏ –∑–∞ –∫–∏–ø—è—Ç–∫–æ–º.\n–í—Ä–∞—á–∏ –æ–±–µ—â–∞—é—Ç –º–Ω–µ –∑–∞–∂–∏–≤–ª–µ–Ω–∏–µ –æ—Ç–≤–µ—Ä—Å—Ç—ã—Ö —Ä–∞–Ω –º–æ–∏—Ö –≤ —Ç–µ—á–µ–Ω–∏–µ –ø–æ–ª—É–≥–æ–¥–∞ –∏ –Ω–∞ —ç—Ç–æ—Ç –ø–µ—Ä–∏–æ–¥ –º–æ–∂–Ω–æ –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞—Ç—å —Å —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ –¥–æ–º–∞—à–Ω–∏–π –æ–±—Ä–∞–∑ –∂–∏–∑–Ω–∏.\n[|]";
        let result = vec![
            PositionalToken { offset: 0, length: 24, token: Token::BBCode { left: vec![
                PositionalToken { offset: 1, length: 5, token: Token::Word("Oxana".to_string()) },
                PositionalToken { offset: 6, length: 1, token: Token::Separator(Separator::Space) },
                PositionalToken { offset: 7, length: 5, token: Token::Word("Putan".to_string()) },
                ], right: vec![
                PositionalToken { offset: 13, length: 10, token: Token::Number(Number::Integer(1712640565)) },
                ] } },
            PositionalToken { offset: 24, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 25, length: 6, token: Token::Word("shared".to_string()) },
            PositionalToken { offset: 31, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 32, length: 1, token: Token::Word("a".to_string()) },
            PositionalToken { offset: 33, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 34, length: 39, token: Token::BBCode { left: vec![
                PositionalToken { offset: 35, length: 4, token: Token::Word("post".to_string()) },
                ], right: vec![
                PositionalToken { offset: 40, length: 32, token: Token::Numerical(Numerical::Alphanumeric("100001150683379_1873048549410150".to_string())) },
                ] } },
            PositionalToken { offset: 73, length: 1, token: Token::Punctuation(".".to_string()) },
            PositionalToken { offset: 74, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 75, length: 1, token: Token::Separator(Separator::Newline) },
            PositionalToken { offset: 76, length: 6, token: Token::Word("Andrew".to_string()) },
            PositionalToken { offset: 82, length: 1, token: Token::Separator(Separator::Newline) },
            PositionalToken { offset: 83, length: 70, token: Token::BBCode { left: vec![
                PositionalToken { offset: 84, length: 4, token: Token::Word("link".to_string()) },
                ], right: vec![
                PositionalToken { offset: 89, length: 63, token: Token::Url("https://www.facebook.com/100001150683379/posts/1873048549410150".to_string()) },
                ] } },
            PositionalToken { offset: 153, length: 1, token: Token::Separator(Separator::Newline) },
            PositionalToken { offset: 154, length: 12, token: Token::Word("–î—Ä—É–∑—å—è".to_string()) },
            PositionalToken { offset: 166, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 167, length: 6, token: Token::Word("–º–æ–∏".to_string()) },
            PositionalToken { offset: 173, length: 1, token: Token::Punctuation(",".to_string()) },
            PositionalToken { offset: 174, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 175, length: 16, token: Token::Word("–∏–∑–¥–∞—Ç–µ–ª–∏".to_string()) },
            PositionalToken { offset: 191, length: 1, token: Token::Punctuation(",".to_string()) },
            PositionalToken { offset: 192, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 193, length: 18, token: Token::Word("—Ä–µ–¥–∞–∫—Ç–æ—Ä—ã".to_string()) },
            PositionalToken { offset: 211, length: 1, token: Token::Punctuation(",".to_string()) },
            PositionalToken { offset: 212, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 213, length: 24, token: Token::Word("–ø—Ä–æ—Å–≤–µ—Ç–∏—Ç–µ–ª–∏".to_string()) },
            PositionalToken { offset: 237, length: 1, token: Token::Punctuation(",".to_string()) },
            PositionalToken { offset: 238, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 239, length: 28, token: Token::Word("–∫—É–ª—å—Ç—É—Ä—Ç—Ä–µ–≥–µ—Ä—ã".to_string()) },
            PositionalToken { offset: 267, length: 1, token: Token::Punctuation(",".to_string()) },
            PositionalToken { offset: 268, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 269, length: 16, token: Token::Word("—Å—É–±—ä–µ–∫—Ç—ã".to_string()) },
            PositionalToken { offset: 285, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 286, length: 16, token: Token::Word("–º–∏—Ä–æ–≤–æ–≥–æ".to_string()) },
            PositionalToken { offset: 302, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 303, length: 10, token: Token::Word("—Ä—ã–Ω–∫–∞".to_string()) },
            PositionalToken { offset: 313, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 314, length: 2, token: Token::Word("–∏".to_string()) },
            PositionalToken { offset: 316, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 317, length: 4, token: Token::Word("—Ç—É".to_string()) },
            PositionalToken { offset: 321, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 322, length: 6, token: Token::Word("—Ö—É–º".to_string()) },
            PositionalToken { offset: 328, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 329, length: 4, token: Token::Word("–∏—Ç".to_string()) },
            PositionalToken { offset: 333, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 334, length: 6, token: Token::Word("–µ—â—ë".to_string()) },
            PositionalToken { offset: 340, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 341, length: 6, token: Token::Word("–º–µ–π".to_string()) },
            PositionalToken { offset: 347, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 348, length: 14, token: Token::Word("–∫–æ–Ω—Å—ë—Ä–Ω".to_string()) },
            PositionalToken { offset: 362, length: 1, token: Token::Punctuation(".".to_string()) },
            PositionalToken { offset: 363, length: 1, token: Token::Separator(Separator::Newline) },
            PositionalToken { offset: 364, length: 4, token: Token::Word("–ù–∞".to_string()) },
            PositionalToken { offset: 368, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 369, length: 14, token: Token::Word("—Ç–µ–∫—É—â–∏–π".to_string()) },
            PositionalToken { offset: 383, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 384, length: 12, token: Token::Word("–º–æ–º–µ–Ω—Ç".to_string()) },
            PositionalToken { offset: 396, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 397, length: 2, token: Token::Word("—è".to_string()) },
            PositionalToken { offset: 399, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 400, length: 10, token: Token::Word("–ª–∏—à–µ–Ω".to_string()) },
            PositionalToken { offset: 410, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 411, length: 10, token: Token::Word("–±—ã–ª–æ–π".to_string()) },
            PositionalToken { offset: 421, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 422, length: 22, token: Token::Word("–ø–æ–¥–≤–∏–∂–Ω–æ—Å—Ç–∏".to_string()) },
            PositionalToken { offset: 444, length: 1, token: Token::Punctuation(",".to_string()) },
            PositionalToken { offset: 445, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 446, length: 8, token: Token::Word("—Ö–æ—Ç—å".to_string()) },
            PositionalToken { offset: 454, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 455, length: 2, token: Token::Word("–∏".to_string()) },
            PositionalToken { offset: 457, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 458, length: 14, token: Token::Word("–∫–æ–≤—ã–ª—è—é".to_string()) },
            PositionalToken { offset: 472, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 473, length: 4, token: Token::Word("–ø–æ".to_string()) },
            PositionalToken { offset: 477, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 478, length: 20, token: Token::Word("–±–æ–ª—å–Ω–∏—á–Ω—ã—Ö".to_string()) },
            PositionalToken { offset: 498, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 499, length: 18, token: Token::Word("–∫–æ—Ä–∏–¥–æ—Ä–∞–º".to_string()) },
            PositionalToken { offset: 517, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 518, length: 4, token: Token::Word("–ø–æ".to_string()) },
            PositionalToken { offset: 522, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 523, length: 12, token: Token::Word("—Ä–∞–∑–Ω—ã–º".to_string()) },
            PositionalToken { offset: 535, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 536, length: 12, token: Token::Word("–Ω—É–∂–¥–∞–º".to_string()) },
            PositionalToken { offset: 548, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 549, length: 2, token: Token::Word("–∏".to_string()) },
            PositionalToken { offset: 551, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 552, length: 4, token: Token::Word("–∑–∞".to_string()) },
            PositionalToken { offset: 556, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 557, length: 16, token: Token::Word("–∫–∏–ø—è—Ç–∫–æ–º".to_string()) },
            PositionalToken { offset: 573, length: 1, token: Token::Punctuation(".".to_string()) },
            PositionalToken { offset: 574, length: 1, token: Token::Separator(Separator::Newline) },
            PositionalToken { offset: 575, length: 10, token: Token::Word("–í—Ä–∞—á–∏".to_string()) },
            PositionalToken { offset: 585, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 586, length: 14, token: Token::Word("–æ–±–µ—â–∞—é—Ç".to_string()) },
            PositionalToken { offset: 600, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 601, length: 6, token: Token::Word("–º–Ω–µ".to_string()) },
            PositionalToken { offset: 607, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 608, length: 20, token: Token::Word("–∑–∞–∂–∏–≤–ª–µ–Ω–∏–µ".to_string()) },
            PositionalToken { offset: 628, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 629, length: 18, token: Token::Word("–æ—Ç–≤–µ—Ä—Å—Ç—ã—Ö".to_string()) },
            PositionalToken { offset: 647, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 648, length: 6, token: Token::Word("—Ä–∞–Ω".to_string()) },
            PositionalToken { offset: 654, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 655, length: 8, token: Token::Word("–º–æ–∏—Ö".to_string()) },
            PositionalToken { offset: 663, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 664, length: 2, token: Token::Word("–≤".to_string()) },
            PositionalToken { offset: 666, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 667, length: 14, token: Token::Word("—Ç–µ—á–µ–Ω–∏–µ".to_string()) },
            PositionalToken { offset: 681, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 682, length: 16, token: Token::Word("–ø–æ–ª—É–≥–æ–¥–∞".to_string()) },
            PositionalToken { offset: 698, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 699, length: 2, token: Token::Word("–∏".to_string()) },
            PositionalToken { offset: 701, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 702, length: 4, token: Token::Word("–Ω–∞".to_string()) },
            PositionalToken { offset: 706, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 707, length: 8, token: Token::Word("—ç—Ç–æ—Ç".to_string()) },
            PositionalToken { offset: 715, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 716, length: 12, token: Token::Word("–ø–µ—Ä–∏–æ–¥".to_string()) },
            PositionalToken { offset: 728, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 729, length: 10, token: Token::Word("–º–æ–∂–Ω–æ".to_string()) },
            PositionalToken { offset: 739, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 740, length: 24, token: Token::Word("–ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞—Ç—å".to_string()) },
            PositionalToken { offset: 764, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 765, length: 2, token: Token::Word("—Å".to_string()) },
            PositionalToken { offset: 767, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 768, length: 24, token: Token::Word("—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é".to_string()) },
            PositionalToken { offset: 792, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 793, length: 30, token: Token::Word("–ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ".to_string()) },
            PositionalToken { offset: 823, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 824, length: 16, token: Token::Word("–¥–æ–º–∞—à–Ω–∏–π".to_string()) },
            PositionalToken { offset: 840, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 841, length: 10, token: Token::Word("–æ–±—Ä–∞–∑".to_string()) },
            PositionalToken { offset: 851, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 852, length: 10, token: Token::Word("–∂–∏–∑–Ω–∏".to_string()) },
            PositionalToken { offset: 862, length: 1, token: Token::Punctuation(".".to_string()) },
            PositionalToken { offset: 863, length: 1, token: Token::Separator(Separator::Newline) },
            PositionalToken { offset: 864, length: 3, token: Token::BBCode { left: vec![
                ], right: vec![
                ] } },
            ];
        let lib_res = uws.into_tokens().unwrap().collect::<Vec<_>>();
        //print_result(&lib_res); panic!("");
        check_results(&result,&lib_res,uws);        
    }


    #[test]
    fn html() {
        let uws = "<div class=\"article article_view \" id=\"article_view_-113039156_9551\" data-article-url=\"/@chaibuket-o-chem-ne-zabyt-25-noyabrya\" data-audio-context=\"article:-113039156_9551\"><h1  class=\"article_decoration_first article_decoration_last\" >–î–µ–Ω—å –ú–∞–º—ã </h1><p  class=\"article_decoration_first article_decoration_last\" >–î–µ–Ω—å, –∫–æ–≥–¥–∞ –ø–æ–∑–¥—Ä–∞–≤–ª—è—é—Ç –º–∞–º, –±–∞–±—É—à–µ–∫, —Å–µ—Å—Ç–µ—Ä –∏ –∂—ë–Ω ‚Äî —ç—Ç–æ –≤—Å–µ–º–∏—Ä–Ω—ã–π –ø—Ä–∞–∑–¥–Ω–∏–∫, –Ω–∞–∑—ã–≤–∞–µ–º—ã–π ¬´–î–µ–Ω—å –ú–∞–º—ã¬ª. –í –Ω–∞—Å—Ç–æ—è—â–µ–µ –≤—Ä–µ–º—è –µ–≥–æ –æ—Ç–º–µ—á–∞—é—Ç –ø–æ—á—Ç–∏ –≤ –∫–∞–∂–¥–æ–π —Å—Ç—Ä–∞–Ω–µ, –ø—Ä–æ—Å—Ç–æ –≤–µ–∑–¥–µ —Ä–∞–∑–Ω—ã–µ –¥–∞—Ç—ã –∏ —Å–ø–æ—Å–æ–±—ã –ø—Ä–∞–∑–¥–Ω–æ–≤–∞–Ω–∏—è. </p><h3  class=\"article_decoration_first article_decoration_last\" ><span class='article_anchor_title'>\n  <span class='article_anchor_button' id='pochemu-my-ego-prazdnuem'></span>\n  <span class='article_anchor_fsymbol'>–ü</span>\n</span>–ü–û–ß–ï–ú–£ –ú–´ –ï–ì–û –ü–†–ê–ó–î–ù–£–ï–ú</h3><p  class=\"article_decoration_first article_decoration_last article_decoration_before\" >–í 1987 –≥–æ–¥—É –∫–æ–º–∏—Ç–µ—Ç –≥–æ—Å–¥—É–º—ã –ø–æ –¥–µ–ª–∞–º –∂–µ–Ω—â–∏–Ω, —Å–µ–º—å–∏ –∏ –º–æ–ª–æ–¥–µ–∂–∏ –≤—ã—Å—Ç—É–ø–∏–ª —Å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ–º —É—á—Ä–µ–¥–∏—Ç—å ¬´–î–µ–Ω—å –º–∞–º—ã¬ª, –∞ —Å–∞–º –ø—Ä–∏–∫–∞–∑ –±—ã–ª –ø–æ–¥–ø–∏—Å–∞–Ω —É–∂–µ 30 —è–Ω–≤–∞—Ä—è 1988 –≥–æ–¥–∞ –ë–æ—Ä–∏—Å–æ–º –ï–ª—å—Ü–∏–Ω—ã–º. –ë—ã–ª–æ —Ä–µ—à–µ–Ω–æ, —á—Ç–æ –µ–∂–µ–≥–æ–¥–Ω–æ –≤ –†–æ—Å—Å–∏–∏ –ø—Ä–∞–∑–¥–Ω–µ—Å—Ç–≤–æ –¥–Ω—è –º–∞–º—ã –±—É–¥–µ—Ç –≤—ã–ø–∞–¥–∞—Ç—å –Ω–∞ –ø–æ—Å–ª–µ–¥–Ω–µ–µ –≤–æ—Å–∫—Ä–µ—Å–µ–Ω—å–µ –Ω–æ—è–±—Ä—è. </p><figure data-type=\"101\" data-mode=\"\"  class=\"article_decoration_first article_decoration_last\" >\n  <div class=\"article_figure_content\" style=\"width: 1125px\">\n    <div class=\"article_figure_sizer_content\"><div class=\"article_object_sizer_wrap\" data-sizes=\"[{&quot;s&quot;:[&quot;https://pp.userapi.com/c849128/v849128704/c0ffd/pcNJaBH3NDo.jpg&quot;,75,50],&quot;m&quot;:[&quot;https://pp.userapi.com/c849128/v849128704/c0ffe/ozCLs2kHtRY.jpg&quot;,130,87],&quot;x&quot;:[&quot;https://pp.userapi.com/c849128/v849128704/c0fff/E4KtTNDydzE.jpg&quot;,604,403],&quot;y&quot;:[&quot;https://pp.userapi.com/c849128/v849128704/c1000/1nLxpYKavzU.jpg&quot;,807,538],&quot;z&quot;:[&quot;https://pp.userapi.com/c849128/v849128704/c1001/IgEODe90yEk.jpg&quot;,1125,750],&quot;o&quot;:[&quot;https://pp.userapi.com/c849128/v849128704/c1002/01faNwVZ2_E.jpg&quot;,130,87],&quot;p&quot;:[&quot;https://pp.userapi.com/c849128/v849128704/c1003/baDFzbdRP2s.jpg&quot;,200,133],&quot;q&quot;:[&quot;https://pp.userapi.com/c849128/v849128704/c1004/CY4khI6KJKA.jpg&quot;,320,213],&quot;r&quot;:[&quot;https://pp.userapi.com/c849128/v849128704/c1005/NOvAJ6-VltY.jpg&quot;,510,340]}]\">\n  <img class=\"article_object_sizer_inner article_object_photo__image_blur\" src=\"https://pp.userapi.com/c849128/v849128704/c0ffd/pcNJaBH3NDo.jpg\" data-baseurl=\"\"/>\n  \n</div></div>\n    <div class=\"article_figure_sizer\" style=\"padding-bottom: 66.666666666667%\"></div>";
        let result = vec![
            PositionalToken { offset: 236, length: 8, token: Token::Word("–î–µ–Ω—å".to_string()) },
            PositionalToken { offset: 244, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 245, length: 8, token: Token::Word("–ú–∞–º—ã".to_string()) },
            PositionalToken { offset: 253, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 321, length: 8, token: Token::Word("–î–µ–Ω—å".to_string()) },
            PositionalToken { offset: 329, length: 1, token: Token::Punctuation(",".to_string()) },
            PositionalToken { offset: 330, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 331, length: 10, token: Token::Word("–∫–æ–≥–¥–∞".to_string()) },
            PositionalToken { offset: 341, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 342, length: 22, token: Token::Word("–ø–æ–∑–¥—Ä–∞–≤–ª—è—é—Ç".to_string()) },
            PositionalToken { offset: 364, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 365, length: 6, token: Token::Word("–º–∞–º".to_string()) },
            PositionalToken { offset: 371, length: 1, token: Token::Punctuation(",".to_string()) },
            PositionalToken { offset: 372, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 373, length: 14, token: Token::Word("–±–∞–±—É—à–µ–∫".to_string()) },
            PositionalToken { offset: 387, length: 1, token: Token::Punctuation(",".to_string()) },
            PositionalToken { offset: 388, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 389, length: 12, token: Token::Word("—Å–µ—Å—Ç–µ—Ä".to_string()) },
            PositionalToken { offset: 401, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 402, length: 2, token: Token::Word("–∏".to_string()) },
            PositionalToken { offset: 404, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 405, length: 6, token: Token::Word("–∂—ë–Ω".to_string()) },
            PositionalToken { offset: 411, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 412, length: 3, token: Token::Unicode("u2014".to_string()) },
            PositionalToken { offset: 415, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 416, length: 6, token: Token::Word("—ç—Ç–æ".to_string()) },
            PositionalToken { offset: 422, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 423, length: 18, token: Token::Word("–≤—Å–µ–º–∏—Ä–Ω—ã–π".to_string()) },
            PositionalToken { offset: 441, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 442, length: 16, token: Token::Word("–ø—Ä–∞–∑–¥–Ω–∏–∫".to_string()) },
            PositionalToken { offset: 458, length: 1, token: Token::Punctuation(",".to_string()) },
            PositionalToken { offset: 459, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 460, length: 20, token: Token::Word("–Ω–∞–∑—ã–≤–∞–µ–º—ã–π".to_string()) },
            PositionalToken { offset: 480, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 481, length: 2, token: Token::Unicode("uab".to_string()) },
            PositionalToken { offset: 483, length: 8, token: Token::Word("–î–µ–Ω—å".to_string()) },
            PositionalToken { offset: 491, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 492, length: 8, token: Token::Word("–ú–∞–º—ã".to_string()) },
            PositionalToken { offset: 500, length: 2, token: Token::Unicode("ubb".to_string()) },
            PositionalToken { offset: 502, length: 1, token: Token::Punctuation(".".to_string()) },
            PositionalToken { offset: 503, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 504, length: 2, token: Token::Word("–í".to_string()) },
            PositionalToken { offset: 506, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 507, length: 18, token: Token::Word("–Ω–∞—Å—Ç–æ—è—â–µ–µ".to_string()) },
            PositionalToken { offset: 525, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 526, length: 10, token: Token::Word("–≤—Ä–µ–º—è".to_string()) },
            PositionalToken { offset: 536, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 537, length: 6, token: Token::Word("–µ–≥–æ".to_string()) },
            PositionalToken { offset: 543, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 544, length: 16, token: Token::Word("–æ—Ç–º–µ—á–∞—é—Ç".to_string()) },
            PositionalToken { offset: 560, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 561, length: 10, token: Token::Word("–ø–æ—á—Ç–∏".to_string()) },
            PositionalToken { offset: 571, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 572, length: 2, token: Token::Word("–≤".to_string()) },
            PositionalToken { offset: 574, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 575, length: 12, token: Token::Word("–∫–∞–∂–¥–æ–π".to_string()) },
            PositionalToken { offset: 587, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 588, length: 12, token: Token::Word("—Å—Ç—Ä–∞–Ω–µ".to_string()) },
            PositionalToken { offset: 600, length: 1, token: Token::Punctuation(",".to_string()) },
            PositionalToken { offset: 601, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 602, length: 12, token: Token::Word("–ø—Ä–æ—Å—Ç–æ".to_string()) },
            PositionalToken { offset: 614, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 615, length: 10, token: Token::Word("–≤–µ–∑–¥–µ".to_string()) },
            PositionalToken { offset: 625, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 626, length: 12, token: Token::Word("—Ä–∞–∑–Ω—ã–µ".to_string()) },
            PositionalToken { offset: 638, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 639, length: 8, token: Token::Word("–¥–∞—Ç—ã".to_string()) },
            PositionalToken { offset: 647, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 648, length: 2, token: Token::Word("–∏".to_string()) },
            PositionalToken { offset: 650, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 651, length: 14, token: Token::Word("—Å–ø–æ—Å–æ–±—ã".to_string()) },
            PositionalToken { offset: 665, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 666, length: 24, token: Token::Word("–ø—Ä–∞–∑–¥–Ω–æ–≤–∞–Ω–∏—è".to_string()) },
            PositionalToken { offset: 690, length: 1, token: Token::Punctuation(".".to_string()) },
            PositionalToken { offset: 691, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 794, length: 1, token: Token::Separator(Separator::Newline) },
            PositionalToken { offset: 795, length: 2, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 870, length: 1, token: Token::Separator(Separator::Newline) },
            PositionalToken { offset: 871, length: 2, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 910, length: 2, token: Token::Word("–ü".to_string()) },
            PositionalToken { offset: 919, length: 1, token: Token::Separator(Separator::Newline) },
            PositionalToken { offset: 927, length: 12, token: Token::Word("–ü–û–ß–ï–ú–£".to_string()) },
            PositionalToken { offset: 939, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 940, length: 4, token: Token::Word("–ú–´".to_string()) },
            PositionalToken { offset: 944, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 945, length: 6, token: Token::Word("–ï–ì–û".to_string()) },
            PositionalToken { offset: 951, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 952, length: 18, token: Token::Word("–ü–†–ê–ó–î–ù–£–ï–ú".to_string()) },
            PositionalToken { offset: 1063, length: 2, token: Token::Word("–í".to_string()) },
            PositionalToken { offset: 1065, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 1066, length: 4, token: Token::Number(Number::Integer(1987)) },
            PositionalToken { offset: 1070, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 1071, length: 8, token: Token::Word("–≥–æ–¥—É".to_string()) },
            PositionalToken { offset: 1079, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 1080, length: 14, token: Token::Word("–∫–æ–º–∏—Ç–µ—Ç".to_string()) },
            PositionalToken { offset: 1094, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 1095, length: 14, token: Token::Word("–≥–æ—Å–¥—É–º—ã".to_string()) },
            PositionalToken { offset: 1109, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 1110, length: 4, token: Token::Word("–ø–æ".to_string()) },
            PositionalToken { offset: 1114, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 1115, length: 10, token: Token::Word("–¥–µ–ª–∞–º".to_string()) },
            PositionalToken { offset: 1125, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 1126, length: 12, token: Token::Word("–∂–µ–Ω—â–∏–Ω".to_string()) },
            PositionalToken { offset: 1138, length: 1, token: Token::Punctuation(",".to_string()) },
            PositionalToken { offset: 1139, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 1140, length: 10, token: Token::Word("—Å–µ–º—å–∏".to_string()) },
            PositionalToken { offset: 1150, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 1151, length: 2, token: Token::Word("–∏".to_string()) },
            PositionalToken { offset: 1153, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 1154, length: 16, token: Token::Word("–º–æ–ª–æ–¥–µ–∂–∏".to_string()) },
            PositionalToken { offset: 1170, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 1171, length: 16, token: Token::Word("–≤—ã—Å—Ç—É–ø–∏–ª".to_string()) },
            PositionalToken { offset: 1187, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 1188, length: 2, token: Token::Word("—Å".to_string()) },
            PositionalToken { offset: 1190, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 1191, length: 24, token: Token::Word("–ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ–º".to_string()) },
            PositionalToken { offset: 1215, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 1216, length: 16, token: Token::Word("—É—á—Ä–µ–¥–∏—Ç—å".to_string()) },
            PositionalToken { offset: 1232, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 1233, length: 2, token: Token::Unicode("uab".to_string()) },
            PositionalToken { offset: 1235, length: 8, token: Token::Word("–î–µ–Ω—å".to_string()) },
            PositionalToken { offset: 1243, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 1244, length: 8, token: Token::Word("–º–∞–º—ã".to_string()) },
            PositionalToken { offset: 1252, length: 2, token: Token::Unicode("ubb".to_string()) },
            PositionalToken { offset: 1254, length: 1, token: Token::Punctuation(",".to_string()) },
            PositionalToken { offset: 1255, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 1256, length: 2, token: Token::Word("–∞".to_string()) },
            PositionalToken { offset: 1258, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 1259, length: 6, token: Token::Word("—Å–∞–º".to_string()) },
            PositionalToken { offset: 1265, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 1266, length: 12, token: Token::Word("–ø—Ä–∏–∫–∞–∑".to_string()) },
            PositionalToken { offset: 1278, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 1279, length: 6, token: Token::Word("–±—ã–ª".to_string()) },
            PositionalToken { offset: 1285, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 1286, length: 16, token: Token::Word("–ø–æ–¥–ø–∏—Å–∞–Ω".to_string()) },
            PositionalToken { offset: 1302, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 1303, length: 6, token: Token::Word("—É–∂–µ".to_string()) },
            PositionalToken { offset: 1309, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 1310, length: 2, token: Token::Number(Number::Integer(30)) },
            PositionalToken { offset: 1312, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 1313, length: 12, token: Token::Word("—è–Ω–≤–∞—Ä—è".to_string()) },
            PositionalToken { offset: 1325, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 1326, length: 4, token: Token::Number(Number::Integer(1988)) },
            PositionalToken { offset: 1330, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 1331, length: 8, token: Token::Word("–≥–æ–¥–∞".to_string()) },
            PositionalToken { offset: 1339, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 1340, length: 14, token: Token::Word("–ë–æ—Ä–∏—Å–æ–º".to_string()) },
            PositionalToken { offset: 1354, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 1355, length: 16, token: Token::Word("–ï–ª—å—Ü–∏–Ω—ã–º".to_string()) },
            PositionalToken { offset: 1371, length: 1, token: Token::Punctuation(".".to_string()) },
            PositionalToken { offset: 1372, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 1373, length: 8, token: Token::Word("–ë—ã–ª–æ".to_string()) },
            PositionalToken { offset: 1381, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 1382, length: 12, token: Token::Word("—Ä–µ—à–µ–Ω–æ".to_string()) },
            PositionalToken { offset: 1394, length: 1, token: Token::Punctuation(",".to_string()) },
            PositionalToken { offset: 1395, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 1396, length: 6, token: Token::Word("—á—Ç–æ".to_string()) },
            PositionalToken { offset: 1402, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 1403, length: 16, token: Token::Word("–µ–∂–µ–≥–æ–¥–Ω–æ".to_string()) },
            PositionalToken { offset: 1419, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 1420, length: 2, token: Token::Word("–≤".to_string()) },
            PositionalToken { offset: 1422, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 1423, length: 12, token: Token::Word("–†–æ—Å—Å–∏–∏".to_string()) },
            PositionalToken { offset: 1435, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 1436, length: 22, token: Token::Word("–ø—Ä–∞–∑–¥–Ω–µ—Å—Ç–≤–æ".to_string()) },
            PositionalToken { offset: 1458, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 1459, length: 6, token: Token::Word("–¥–Ω—è".to_string()) },
            PositionalToken { offset: 1465, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 1466, length: 8, token: Token::Word("–º–∞–º—ã".to_string()) },
            PositionalToken { offset: 1474, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 1475, length: 10, token: Token::Word("–±—É–¥–µ—Ç".to_string()) },
            PositionalToken { offset: 1485, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 1486, length: 16, token: Token::Word("–≤—ã–ø–∞–¥–∞—Ç—å".to_string()) },
            PositionalToken { offset: 1502, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 1503, length: 4, token: Token::Word("–Ω–∞".to_string()) },
            PositionalToken { offset: 1507, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 1508, length: 18, token: Token::Word("–ø–æ—Å–ª–µ–¥–Ω–µ–µ".to_string()) },
            PositionalToken { offset: 1526, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 1527, length: 22, token: Token::Word("–≤–æ—Å–∫—Ä–µ—Å–µ–Ω—å–µ".to_string()) },
            PositionalToken { offset: 1549, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 1550, length: 12, token: Token::Word("–Ω–æ—è–±—Ä—è".to_string()) },
            PositionalToken { offset: 1562, length: 1, token: Token::Punctuation(".".to_string()) },
            PositionalToken { offset: 1563, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 1664, length: 1, token: Token::Separator(Separator::Newline) },
            PositionalToken { offset: 1665, length: 2, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 1725, length: 1, token: Token::Separator(Separator::Newline) },
            PositionalToken { offset: 1726, length: 4, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 2725, length: 1, token: Token::Separator(Separator::Newline) },
            PositionalToken { offset: 2726, length: 2, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 2888, length: 1, token: Token::Separator(Separator::Newline) },
            PositionalToken { offset: 2889, length: 2, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 2891, length: 1, token: Token::Separator(Separator::Newline) },
            PositionalToken { offset: 2904, length: 1, token: Token::Separator(Separator::Newline) },
            PositionalToken { offset: 2905, length: 4, token: Token::Separator(Separator::Space) },
            ];
        match uws.into_tokens() {
            Err(Untokenizable::Html) => {},
            _ => panic!("Untokenizable::Html"),
        }
        //let lib_res = uws.into_tokens().unwrap().collect::<Vec<_>>();
        //check_results(&result,&lib_res,uws);
        //print_result(&lib_res); panic!("")
    }

    #[test]
    fn vk_bbcode() {
        let uws = "[club113623432|üíúüíúüíú - –¥–ª—è –¥–µ–≤—É—à–µ–∫] \n[club113623432|üíõüíõüíõ - –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ–∫]";
        let result = vec![
            PositionalToken { offset: 0, length: 52, token: Token::BBCode { left: vec![
                PositionalToken { offset: 1, length: 13, token: Token::Numerical(Numerical::Alphanumeric("club113623432".to_string())) },
                ], right: vec![
                PositionalToken { offset: 15, length: 4, token: Token::Emoji("purple_heart".to_string()) },
                PositionalToken { offset: 19, length: 4, token: Token::Emoji("purple_heart".to_string()) },
                PositionalToken { offset: 23, length: 4, token: Token::Emoji("purple_heart".to_string()) },
                PositionalToken { offset: 27, length: 1, token: Token::Separator(Separator::Space) },
                PositionalToken { offset: 28, length: 1, token: Token::Punctuation("-".to_string()) },
                PositionalToken { offset: 29, length: 1, token: Token::Separator(Separator::Space) },
                PositionalToken { offset: 30, length: 6, token: Token::Word("–¥–ª—è".to_string()) },
                PositionalToken { offset: 36, length: 1, token: Token::Separator(Separator::Space) },
                PositionalToken { offset: 37, length: 14, token: Token::Word("–¥–µ–≤—É—à–µ–∫".to_string()) },
                ] } },
            PositionalToken { offset: 52, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 53, length: 1, token: Token::Separator(Separator::Newline) },
            PositionalToken { offset: 54, length: 58, token: Token::BBCode { left: vec![
                PositionalToken { offset: 55, length: 13, token: Token::Numerical(Numerical::Alphanumeric("club113623432".to_string())) },
                ], right: vec![
                PositionalToken { offset: 69, length: 4, token: Token::Emoji("yellow_heart".to_string()) },
                PositionalToken { offset: 73, length: 4, token: Token::Emoji("yellow_heart".to_string()) },
                PositionalToken { offset: 77, length: 4, token: Token::Emoji("yellow_heart".to_string()) },
                PositionalToken { offset: 81, length: 1, token: Token::Separator(Separator::Space) },
                PositionalToken { offset: 82, length: 1, token: Token::Punctuation("-".to_string()) },
                PositionalToken { offset: 83, length: 1, token: Token::Separator(Separator::Space) },
                PositionalToken { offset: 84, length: 6, token: Token::Word("–¥–ª—è".to_string()) },
                PositionalToken { offset: 90, length: 1, token: Token::Separator(Separator::Space) },
                PositionalToken { offset: 91, length: 20, token: Token::Word("—Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ–∫".to_string()) },
                ] } },
            ];
        let lib_res = uws.into_tokens().unwrap().collect::<Vec<_>>();
        //print_result(&lib_res); panic!("");
        check_results(&result,&lib_res,uws);
    }

    /*#[test]
    fn text_href_and_html () {
        let uws = "https://youtu.be/dQErLQZw3qA</a></p><figure data-type=\"102\" data-mode=\"\"  class=\"article_decoration_first article_decoration_last\" >\n";
        let result =  vec![
            PositionalToken { offset: 0, length: 28, token: Token::Url("https://youtu.be/dQErLQZw3qA".to_string()) },
            PositionalToken { offset: 132, length: 1, token: Token::Separator(Separator::Newline) },
            ];
        let lib_res = uws.into_tokens().unwrap().collect::<Vec<_>>();
        check_results(&result,&lib_res,uws);
        //print_result(&lib_res); panic!("")
    }*/

    #[test]
    fn numerical() {
        let uws = "12.02.18 31.28.34 23.11.2018 123.568.365.234.578 127.0.0.1 1st 1–∫–≥ 123123–∞—Ñ—ã–≤–∞—ã–≤ 12321—Ñ–≤–∞—Ñ—ã–æ–≤234–≤—ã–∞–ª—Ñ–æ 12_123_343.4234_4234";
        let lib_res = uws.into_tokens().unwrap().collect::<Vec<_>>();
        //print_result(&lib_res); panic!("");
        let result = vec![
            PositionalToken { offset: 0, length: 8, token: Token::Numerical(Numerical::DotSeparated("12.02.18".to_string())) },
            PositionalToken { offset: 8, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 9, length: 8, token: Token::Numerical(Numerical::DotSeparated("31.28.34".to_string())) },
            PositionalToken { offset: 17, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 18, length: 10, token: Token::Numerical(Numerical::DotSeparated("23.11.2018".to_string())) },
            PositionalToken { offset: 28, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 29, length: 19, token: Token::Numerical(Numerical::DotSeparated("123.568.365.234.578".to_string())) },
            PositionalToken { offset: 48, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 49, length: 9, token: Token::Numerical(Numerical::DotSeparated("127.0.0.1".to_string())) },
            PositionalToken { offset: 58, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 59, length: 3, token: Token::Numerical(Numerical::Measures("1st".to_string())) },
            PositionalToken { offset: 62, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 63, length: 5, token: Token::Numerical(Numerical::Measures("1–∫–≥".to_string())) },
            PositionalToken { offset: 68, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 69, length: 20, token: Token::Numerical(Numerical::Measures("123123–∞—Ñ—ã–≤–∞—ã–≤".to_string())) },
            PositionalToken { offset: 89, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 90, length: 34, token: Token::Numerical(Numerical::Alphanumeric("12321—Ñ–≤–∞—Ñ—ã–æ–≤234–≤—ã–∞–ª—Ñ–æ".to_string())) },
            PositionalToken { offset: 124, length: 1, token: Token::Separator(Separator::Space) },
            PositionalToken { offset: 125, length: 20, token: Token::Numerical(Numerical::Alphanumeric("12_123_343.4234_4234".to_string())) },
            ];
        check_results(&result,&lib_res,uws);
       
    }

        /*#[test]
    fn new_test() {
        let uws = "";
        let lib_res = uws.into_tokens().unwrap().collect::<Vec<_>>();
        print_result(&lib_res); panic!("");
        let result = vec![];
        check_results(&result,&lib_res,uws);
        
}*/




    /* Language tests */

    enum Lang {
        Zho,
        Jpn,
        Kor,
        Ara,
        Ell,
    }

    #[test]
    fn test_lang_zho() {
        let (uws,result) = get_lang_test(Lang::Zho);
        let lib_res = uws.into_tokens().unwrap().collect::<Vec<_>>();
        check_results(&result,&lib_res,&uws);
    }

    #[test]
    fn test_lang_jpn() {
        let (uws,result) = get_lang_test(Lang::Jpn);
        let lib_res = uws.into_tokens().unwrap().collect::<Vec<_>>();
        check_results(&result,&lib_res,&uws);
    }

    #[test]
    fn test_lang_kor() {
        let (uws,result) = get_lang_test(Lang::Kor);
        let lib_res = uws.into_tokens().unwrap().collect::<Vec<_>>();
        check_results(&result,&lib_res,&uws);
    }

    #[test]
    fn test_lang_ara() {
        let (uws,result) = get_lang_test(Lang::Ara);
        let lib_res = uws.into_tokens().unwrap().collect::<Vec<_>>();
        check_results(&result,&lib_res,&uws);
    }

    #[test]
    fn test_lang_ell() {
        let (uws,result) = get_lang_test(Lang::Ell);
        let lib_res = uws.into_tokens().unwrap().collect::<Vec<_>>();
        check_results(&result,&lib_res,&uws);
    }

    fn get_lang_test(lng: Lang) -> (String, Vec<PositionalToken>) {
        let text = match lng {
            Lang::Zho => "ÁæéÂõΩÁîµËßÜËøûÁª≠Ââß„ÄäË∂Ö‰∫∫Ââç‰º†„ÄãÁöÑÁ¨¨‰∏ÄÈõÜ„ÄäËØïÊí≠ÈõÜ„Äã‰∫é2001Âπ¥10Êúà16Êó•Âú®ÈõªË¶ñÁ∂≤È¶ñÊí≠ÔºåÂâßÈõÜ‰∏ªÂàõ‰∫∫ÈòøÂ∞îÂºóÈõ∑Âæ∑¬∑È´òÂ§´ÂíåËøàÂ∞îÊñØ¬∑Á±≥ÂãíÁ∑®ÂäáÔºåÂ§ßÂç´¬∑Âä™ÁâπÂ∞îÊâßÂØº„ÄÇËøô‰∏ÄËØïÊí≠È¶ñÊ¨°ÂêëËßÇ‰ºóÂºïËçê‰∫ÜÂÖãÊãâÂÖã¬∑ËÇØÁâπ‰∏ÄËßíÔºå‰ªñÊòØ‰ΩçÊã•ÊúâË∂ÖËÉΩÂäõÁöÑÂ§ñÊòüÂ≠§ÂÑøÔºå‰∏éÂÆ∂‰∫∫ÂíåÊúãÂèã‰∏ÄËµ∑Âú®Â†™Ëñ©ÊñØÂ∑ûËôöÊûÑÂ∞èÈïáÊñØËé´Áª¥Â∞îÁîüÊ¥ª„ÄÇÂú®Ëøô‰∏ÄÈõÜÈáåÔºåËÇØÁâπÈ¶ñÂ∫¶ÂæóÁü•Ëá™Â∑±ÁöÑÊù•ÂéÜÔºåÂêåÊó∂ËøòÈúÄË¶ÅÈòªÊ≠¢‰∏Ä‰ΩçÂ≠¶ÁîüËØïÂõæÊùÄÊ≠ªÈïá‰∏äÈ´ò‰∏≠Â§öÂêçÂ≠¶ÁîüÁöÑÊä•Â§ç‰πã‰∏æ„ÄÇÊú¨ÈõÜËäÇÁõÆÈáåÂºïÂÖ•‰∫ÜÂ§ö‰∏™‰πãÂêéÂ∞ÜË¥ØÁ©øÂÖ®Â≠£ÁîöËá≥Êï¥ÈÉ®ÂâßÈõÜÁöÑ‰∏ªÈ¢òÂÖÉÁ¥†Ôºå‰æãÂ¶ÇÂá†‰Ωç‰∏ªË¶ÅËßíËâ≤‰πãÈó¥ÁöÑ‰∏âËßíÊÅãÊÉÖ„ÄÇÁîµËßÜÂâßÂú®Âä†ÊãøÂ§ßÊ∫´Âì•ËèØÂèñÊôØÔºåÊó®Âú®ÈÄâÁî®ÂÖ∂‚ÄúÁæéÂõΩ‰∏≠‰∫ßÈò∂Á∫ß‚ÄùÊôØËßÇÔºå‰∏ªÂàõ‰∫∫Ëä±‰∫Ü5‰∏™ÊúàÁöÑÊó∂Èó¥‰∏ìÈó®Áî®‰∫é‰∏∫‰∏ªËßíÁâ©Ëâ≤ÂêàÈÄÇÁöÑÊºîÂëò„ÄÇËØïÊí≠ÈõÜÂú®ÊâÄÊúâÊºîÂëòÈÄâÂ•Ω4Â§©ÂêéÊ≠£ÂºèÂºÄÊãç„ÄÇÁî±‰∫éÊó∂Èó¥‰∏äÁöÑÈôêÂà∂ÔºåÂâßÁªÑÊó†Ê≥ïÊê≠Âª∫Â•ΩÂÆû‰ΩìÂ§ñÊôØÔºåÂõ†Ê≠§Âè™ËÉΩ‰ΩøÁî®ËÆ°ÁÆóÊú∫ÁªòÂõæÊäÄÊúØÂ∞ÜÊï∞Â≠óÂåñÁöÑÂ§ñÊôØÊèíÂÖ•Âà∞ÈïúÂ§¥‰∏≠„ÄÇËäÇÁõÆ‰∏ÄÁªè‰∏äÊò†Â∞±ÊâìÁ†¥‰∫ÜÁîµËßÜÁΩëÁöÑÂ§öÈ°πÊî∂ËßÜÁ∫™ÂΩïÔºåÂπ∂‰∏îËé∑Âæó‰∫ÜËØÑËÆ∫ÂëòÁöÑÊôÆÈÅçÂ•ΩËØÑÂíåÂ§ö‰∏™Â•ñÈ°πÊèêÂêçÔºåÂπ∂Âú®ÂÖ∂‰∏≠‰∏§È°π‰∏äËÉúÂá∫",
            Lang::Kor =>  "ÌîåÎ†àÏù¥Ïä§ÌÖåÏù¥ÏÖò ÏùÄ ÏÜåÎãà Ïª¥Ìì®ÌÑ∞ ÏóîÌÑ∞ÌÖåÏù∏Î®ºÌä∏Í∞Ä Í∞úÎ∞úÌïú ÏÑ∏ Î≤àÏß∏ Í∞ÄÏ†ïÏö© Í≤åÏûÑÍ∏∞Ïù¥Îã§. ÎßàÏù¥ÌÅ¨Î°úÏÜåÌîÑÌä∏Ïùò ÏóëÏä§Î∞ïÏä§ 360, ÎãåÌÖêÎèÑÏùò WiiÏôÄ Í≤ΩÏüÅÌïòÍ≥† ÏûàÎã§. Ïù¥Ï†Ñ Ï†úÌíàÏóêÏÑú Ïò®ÎùºÏù∏ ÌîåÎ†àÏù¥ Í∏∞Îä•ÏùÑ ÎπÑÎîîÏò§ Í≤åÏûÑ Í∞úÎ∞úÏÇ¨Ïóê Ï†ÑÏ†ÅÏúºÎ°ú ÏùòÏ°¥ÌïòÎçò Í≤ÉÍ≥º Îã¨Î¶¨ ÌÜµÌï© Ïò®ÎùºÏù∏ Í≤åÏûÑ ÏÑúÎπÑÏä§Ïù∏ ÌîåÎ†àÏù¥Ïä§ÌÖåÏù¥ÏÖò ÎÑ§Ìä∏ÏõåÌÅ¨ ÏÑúÎπÑÏä§Î•º Î∞úÎß§ÏôÄ Ìï®Íªò ÏãúÏûëÌï¥ Ï†úÍ≥µÌïòÍ≥† ÏûàÏúºÎ©∞, ÌÉÑÌÉÑÌïú Î©ÄÌã∞ÎØ∏ÎîîÏñ¥ Ïû¨ÏÉù Í∏∞Îä•, ÌîåÎ†àÏù¥Ïä§ÌÖåÏù¥ÏÖò Ìè¨ÌÑ∞Î∏îÍ≥ºÏùò Ïó∞Í≤∞, Í≥†ÌôîÏßà Í¥ëÌïô ÎîîÏä§ÌÅ¨ Ìè¨Îß∑Ïù∏ Î∏îÎ£®Î†àÏù¥ ÎîîÏä§ÌÅ¨ Ïû¨ÏÉù Í∏∞Îä• Îì±Ïùò Í∏∞Îä•ÏùÑ Í∞ñÏ∂îÍ≥† ÏûàÎã§. 2006ÎÖÑ 11Ïõî 11ÏùºÏóê ÏùºÎ≥∏ÏóêÏÑú Ï≤òÏùåÏúºÎ°ú Ï∂úÏãúÌñàÏúºÎ©∞, 11Ïõî 17ÏùºÏóêÎäî Î∂ÅÎØ∏ ÏßÄÏó≠, 2007ÎÖÑ 3Ïõî 23ÏùºÏóêÎäî Ïú†ÎüΩÍ≥º Ïò§ÏÑ∏ÏïÑÎãàÏïÑ ÏßÄÏó≠ÏóêÏÑú, ÎåÄÌïúÎØºÍµ≠Ïùò Í≤ΩÏö∞ 6Ïõî 5ÏùºÎ∂ÄÌÑ∞ ÏùºÏ£ºÏùºÍ∞Ñ ÏòàÏïΩÌåêÎß§Î•º Ïã§ÏãúÌï¥, Îß§Ïùº Ï§ÄÎπÑÌïú ÏàòÎüâÏù¥ ÎèôÏù¥ ÎÇòÎäî Îì± ÎßéÏùÄ Í¥ÄÏã¨ÏùÑ Î∞õÏïòÏúºÎ©∞ 6Ïõî 16ÏùºÏóê Ï†ïÏãù Ï∂úÏãú ÌñâÏÇ¨Î•º Ïó¥ÏóàÎã§",
            Lang::Jpn => "ÁÜäÈáé‰∏âÂ±±Êú¨È°òÊâÄ„ÅØ„ÄÅ15‰∏ñÁ¥ÄÊú´‰ª•Èôç„Å´„Åä„Åë„ÇãÁÜäÈáé‰∏âÂ±±ÔºàÁÜäÈáéÊú¨ÂÆÆ„ÄÅÁÜäÈáéÊñ∞ÂÆÆ„ÄÅÁÜäÈáéÈÇ£Êô∫Ôºâ„ÅÆÈÄ†Âñ∂„Éª‰øÆÈÄ†„ÅÆ„Åü„ÇÅ„ÅÆÂãßÈÄ≤„ÇíÊãÖ„Å£„ÅüÁµÑÁπî„ÅÆÁ∑èÁß∞„ÄÇ ÁÜäÈáé‰∏âÂ±±„ÇíÂê´„ÇÅ„Å¶„ÄÅÊó•Êú¨„Å´„Åä„Åë„ÇãÂè§‰ª£„Åã„Çâ‰∏≠‰∏ñÂâçÂçä„Å´„Åã„Åë„Å¶„ÅÆÂØ∫Á§æ„ÅÆÈÄ†Âñ∂„ÅØ„ÄÅÂØ∫Á§æÈ†òÁµåÂñ∂„ÅÆ„Çà„ÅÜ„Å™ÊÅíÂ∏∏ÁöÑË≤°Ê∫ê„ÄÅÂπïÂ∫ú„ÇÑÊúùÂª∑„Å™„Å©„Åã„Çâ„ÅÆ‰∏ÄÊôÇÁöÑ„Å™ÈÄ†Âñ∂ÊñôÊâÄ„ÅÆÂØÑÈÄ≤„ÄÅ„ÅÇ„Çã„ÅÑ„ÅØÂÖ¨Ê®©Âäõ„Åã„Çâ„ÅÆËá®ÊôÇ„ÅÆ‰øùË≠∑„Å´„Çà„Å£„Å¶Ë°å„Çè„Çå„Å¶„ÅÑ„Åü„ÄÇ„Åó„Åã„Åó„Å™„Åå„Çâ„ÄÅÁÜäÈáé‰∏âÂ±±„Åß„ÅØ„ÄÅ„Åì„Çå„Çâ„ÅÆË≤°Ê∫ê„ÅØ„Åô„Åπ„Å¶15‰∏ñÁ¥ÄÂçä„Å∞„Åæ„Åß„Å´ÂÆüÂäπÊÄß„ÇíÂ§±„Å£„Åü",
            Lang::Ara => "ŸÑÿ¥⁄©ÿ±⁄©ÿ¥€å‚ÄåŸáÿß€å ÿ±Ÿàÿ≥‚ÄåŸáÿß€å Ÿàÿßÿ±ŸÜ⁄Ø€å ÿ®Ÿá ÿØÿ±€åÿß€å ÿÆÿ≤ÿ± ŸÖÿ¨ŸÖŸàÿπŸá‚Äåÿß€å ÿßÿ≤ ÿ≠ŸÖŸÑÿßÿ™ ŸÜÿ∏ÿßŸÖ€å ÿØÿ± ÿ®€åŸÜ ÿ≥ÿßŸÑ‚ÄåŸáÿß€å €∏€∂€¥ ÿ™ÿß €±€∞€¥€± ŸÖ€åŸÑÿßÿØ€å ÿ®Ÿá ÿ≥Ÿàÿßÿ≠ŸÑ ÿØÿ±€åÿß€å ÿÆÿ≤ÿ± ÿ®ŸàÿØŸá‚Äåÿßÿ≥ÿ™. ÿ±Ÿàÿ≥‚ÄåŸáÿß€å Ÿàÿßÿ±ŸÜ⁄Ø€å ÿßÿ®ÿ™ÿØÿß ÿØÿ± ŸÇÿ±ŸÜ ŸÜŸáŸÖ ŸÖ€åŸÑÿßÿØ€å ÿ®Ÿá ÿπŸÜŸàÿßŸÜ ÿ®ÿßÿ≤ÿ±⁄ØÿßŸÜÿßŸÜ ŸæŸàÿ≥ÿ™ÿå ÿπÿ≥ŸÑ Ÿà ÿ®ÿ±ÿØŸá ÿØÿ± ÿ≥ÿ±ÿ≤ŸÖ€åŸÜ‚ÄåŸáÿß€å ÿßÿ≥ŸÑÿßŸÖ€å(ÿ≥ÿ±⁄©ŸÑŸÜÿØ) ÿ∏ÿßŸáÿ± ÿ¥ÿØŸÜÿØ. ÿß€åŸÜ ÿ®ÿßÿ≤ÿ±⁄ØÿßŸÜÿßŸÜ ÿØÿ± ŸÖÿ≥€åÿ± ÿ™ÿ¨ÿßÿ±€å ŸàŸÑ⁄Øÿß ÿ®Ÿá ÿÆÿ±€åÿØ Ÿà ŸÅÿ±Ÿàÿ¥ ŸÖ€å‚ÄåŸæÿ±ÿØÿßÿÆÿ™ŸÜÿØ. ŸÜÿÆÿ≥ÿ™€åŸÜ ÿ≠ŸÖŸÑŸáŸî ÿ¢ŸÜÿßŸÜ ÿØÿ± ŸÅÿßÿµŸÑŸá ÿ≥ÿßŸÑ‚ÄåŸáÿß€å €∏€∂€¥ ÿ™ÿß €∏€∏€¥ ŸÖ€åŸÑÿßÿØ€å ÿØÿ± ŸÖŸÇ€åÿßÿ≥€å ⁄©Ÿà⁄Ü⁄© ÿπŸÑ€åŸá ÿπŸÑŸà€åÿßŸÜ ÿ∑ÿ®ÿ±ÿ≥ÿ™ÿßŸÜ ÿ±ÿÆ ÿØÿßÿØ. ŸÜÿÆÿ≥ÿ™€åŸÜ €åŸàÿ±ÿ¥ ÿ®ÿ≤ÿ±⁄Ø ÿ±Ÿàÿ≥‚ÄåŸáÿß ÿØÿ± ÿ≥ÿßŸÑ €π€±€≥ ÿ±ÿÆ ÿØÿßÿØ Ÿà ÿ¢ŸÜÿßŸÜ ÿ®ÿß €µ€∞€∞ ŸÅÿ±ŸàŸÜÿØ ÿØÿ±ÿßÿ≤⁄©ÿ¥ÿ™€å ÿ¥Ÿáÿ± ⁄Øÿ±⁄ØÿßŸÜ Ÿà ÿßÿ∑ÿ±ÿßŸÅ ÿ¢ŸÜ ÿ±ÿß ÿ∫ÿßÿ±ÿ™ ⁄©ÿ±ÿØŸÜÿØ. ÿ¢ŸÜ‚ÄåŸáÿß ÿØÿ± ÿß€åŸÜ ÿ≠ŸÖŸÑŸá ŸÖŸÇÿØÿßÿ±€å ⁄©ÿßŸÑÿß Ÿà ÿ®ÿ±ÿØŸá ÿ±ÿß ÿ®Ÿá ÿ™ÿßÿ±ÿßÿ¨ ÿ®ÿ±ÿØŸÜÿØ Ÿà ÿØÿ± ÿ±ÿßŸá ÿ®ÿßÿ≤⁄Øÿ¥ÿ™ŸÜ ÿ®Ÿá ÿ≥ŸÖÿ™ ÿ¥ŸÖÿßŸÑÿå ÿØÿ± ÿØŸÑÿ™ÿß€å ŸàŸÑ⁄Øÿßÿå ŸÖŸàÿ±ÿØ ÿ≠ŸÖŸÑŸáŸî ÿÆÿ≤ÿ±Ÿáÿß€å ŸÖÿ≥ŸÑŸÖÿßŸÜ ŸÇÿ±ÿßÿ± ⁄Øÿ±ŸÅÿ™ŸÜÿØ Ÿà ÿ®ÿπÿ∂€å ÿßÿ≤ ÿ¢ŸÜÿßŸÜ ŸÖŸàŸÅŸÇ ÿ®Ÿá ŸÅÿ±ÿßÿ± ÿ¥ÿØŸÜÿØÿå ŸàŸÑ€å ÿØÿ± ŸÖ€åÿßŸÜŸáŸî ŸàŸÑ⁄Øÿß ÿ®Ÿá ŸÇÿ™ŸÑ ÿ±ÿ≥€åÿØŸÜÿØ. ÿØŸàŸÖ€åŸÜ Ÿáÿ¨ŸàŸÖ ÿ®ÿ≤ÿ±⁄Ø ÿ±Ÿàÿ≥‚ÄåŸáÿß ÿ®Ÿá ÿØÿ±€åÿß€å ÿÆÿ≤ÿ± ÿØÿ± ÿ≥ÿßŸÑ €π€¥€≥ ÿ®Ÿá ŸàŸÇŸàÿπ Ÿæ€åŸàÿ≥ÿ™. ÿØÿ± ÿß€åŸÜ ÿØŸàÿ±Ÿá ÿß€å⁄ØŸàÿ± €å⁄©ŸÖÿå ÿ≠ÿß⁄©ŸÖ ÿ±Ÿàÿ≥ ⁄©€åŸÅÿå ÿ±Ÿáÿ®ÿ±€å ÿ±Ÿàÿ≥‚ÄåŸáÿß ÿ±ÿß ÿØÿ± ÿØÿ≥ÿ™ ÿØÿßÿ¥ÿ™. ÿ±Ÿàÿ≥‚ÄåŸáÿß Ÿæÿ≥ ÿßÿ≤ ÿ™ŸàÿßŸÅŸÇ ÿ®ÿß ÿØŸàŸÑÿ™ ÿÆÿ≤ÿ±Ÿáÿß ÿ®ÿ±ÿß€å ÿπÿ®Ÿàÿ± ÿßŸÖŸÜ ÿßÿ≤ ŸÖŸÜÿ∑ŸÇŸáÿå ÿ™ÿß ÿ±ŸàÿØ ⁄©Ÿàÿ±ÿß Ÿà ÿßÿπŸÖÿßŸÇ ŸÇŸÅŸÇÿßÿ≤ Ÿæ€åÿ¥ ÿ±ŸÅÿ™ŸÜÿØ Ÿà ÿØÿ± ÿ≥ÿßŸÑ €π€¥€≥ ŸÖŸàŸÅŸÇ ÿ¥ÿØŸÜÿØ ÿ®ŸÜÿØÿ± ÿ®ÿ±ÿØÿπŸáÿå Ÿæÿß€åÿ™ÿÆÿ™ ÿßÿ±ÿßŸÜ (ÿ¨ŸÖŸáŸàÿ±€å ÿ¢ÿ∞ÿ±ÿ®ÿß€åÿ¨ÿßŸÜ ⁄©ŸÜŸàŸÜ€å)ÿå ÿ±ÿß ÿ™ÿµÿ±ŸÅ ⁄©ŸÜŸÜÿØ. ÿ±Ÿàÿ≥‚ÄåŸáÿß ÿØÿ± ÿ¢ŸÜÿ¨ÿß ÿ®Ÿá ŸÖÿØÿ™ ⁄ÜŸÜÿØ ŸÖÿßŸá ŸÖÿßŸÜÿØŸÜÿØ Ÿà ÿ®ÿ≥€åÿßÿ±€å ÿßÿ≤ ÿ≥ÿß⁄©ŸÜÿßŸÜ ÿ¥Ÿáÿ± ÿ±ÿß ⁄©ÿ¥ÿ™ŸÜÿØ Ÿà ÿßÿ≤ ÿ±ÿßŸá ÿ∫ÿßÿ±ÿ™‚Äå⁄Øÿ±€å ÿßŸÖŸàÿßŸÑ€å ÿ±ÿß ÿ®Ÿá ÿ™ÿßÿ±ÿßÿ¨ ÿ®ÿ±ÿØŸÜÿØ. ÿ™ŸÜŸáÿß ÿØŸÑ€åŸÑ ÿ®ÿßÿ≤⁄Øÿ¥ÿ™ ÿ¢ŸÜÿßŸÜ ",
            Lang::Ell => "Œ§Œø Œ†œÅœåŒ≥œÅŒ±ŒºŒºŒ± œÖŒªŒøœÄŒøŒπŒµŒØœÑŒ±Œπ ŒµŒæ ŒøŒªŒøŒ∫ŒªŒÆœÅŒøœÖ Œ±œÄœå Œ±œÄœåœÉœÑŒ±œÉŒ∑ Œ∫Œ±Œπ ŒºœÄŒøœÅŒµŒØ ŒΩŒ± œÉœÖŒºŒºŒµœÑŒ≠œáŒµŒπ Œ∫Œ¨Œ∏Œµ ŒµŒºœÄŒªŒµŒ∫œåŒºŒµŒΩŒøœÇ œÉœÑŒ∑ ŒÆ/Œ∫Œ±Œπ ŒµŒΩŒ¥ŒπŒ±œÜŒµœÅœåŒºŒµŒΩŒøœÇ Œ≥ŒπŒ± œÑŒ∑ Œ¥ŒπŒ¥Œ±œÉŒ∫Œ±ŒªŒØŒ± œÑŒ∑œÇ ŒïŒªŒªŒ∑ŒΩŒπŒ∫ŒÆœÇ œâœÇ Œ¥ŒµœçœÑŒµœÅŒ∑œÇ/ŒæŒ≠ŒΩŒ∑œÇ Œ≥ŒªœéœÉœÉŒ±œÇ œÉœÑŒ∑ŒΩ ŒïŒªŒªŒ¨Œ¥Œ± Œ∫Œ±Œπ œÉœÑŒø ŒµŒæœâœÑŒµœÅŒπŒ∫œå, Œ±œÅŒ∫ŒµŒØ ŒΩŒ± ŒµŒØŒΩŒ±Œπ Œ±œÄœåœÜŒøŒπœÑŒøœÇ ŒµŒªŒªŒ∑ŒΩŒπŒ∫ŒÆœÇ œÜŒπŒªŒøŒªŒøŒ≥ŒØŒ±œÇ, ŒæŒ≠ŒΩœâŒΩ œÜŒπŒªŒøŒªŒøŒ≥ŒπœéŒΩ, œÄŒ±ŒπŒ¥Œ±Œ≥œâŒ≥ŒπŒ∫œéŒΩ œÑŒºŒ∑ŒºŒ¨œÑœâŒΩ, Œ∏ŒµŒøŒªŒøŒ≥ŒπŒ∫œéŒΩ œÉœáŒøŒªœéŒΩ ŒÆ Œ¨ŒªŒªœâŒΩ œÄŒ±ŒΩŒµœÄŒπœÉœÑŒ∑ŒºŒπŒ±Œ∫œéŒΩ œÑŒºŒ∑ŒºŒ¨œÑœâŒΩ ŒµŒªŒªŒ∑ŒΩŒπŒ∫œéŒΩ ŒÆ ŒπœÉœåœÑŒπŒºœâŒΩ ŒæŒ≠ŒΩœâŒΩ œÄŒ±ŒΩŒµœÄŒπœÉœÑŒ∑ŒºŒØœâŒΩ. Œ•œÄœå œåœÅŒøœÖœÇ Œ≥ŒØŒΩŒøŒΩœÑŒ±Œπ Œ¥ŒµŒ∫œÑŒøŒØ œÖœÄŒøœàŒÆœÜŒπŒøŒπ œÄŒøœÖ Œ¥ŒµŒΩ Œ≠œáŒøœÖŒΩ ŒøŒªŒøŒ∫ŒªŒ∑œÅœéœÉŒµŒπ œÉœÄŒøœÖŒ¥Œ≠œÇ œÑœÅŒπœÑŒøŒ≤Œ¨Œ∏ŒºŒπŒ±œÇ ŒµŒ∫œÄŒ±ŒØŒ¥ŒµœÖœÉŒ∑œÇ.",
        }.chars().take(100).fold(String::new(),|acc,c| acc + &format!("{}",c));
        let tokens = match lng {
            Lang::Zho => vec![
                PositionalToken { offset: 0, length: 3, token: Token::Word("Áæé".to_string()) },
                PositionalToken { offset: 3, length: 3, token: Token::Word("ÂõΩ".to_string()) },
                PositionalToken { offset: 6, length: 3, token: Token::Word("Áîµ".to_string()) },
                PositionalToken { offset: 9, length: 3, token: Token::Word("ËßÜ".to_string()) },
                PositionalToken { offset: 12, length: 3, token: Token::Word("Ëøû".to_string()) },
                PositionalToken { offset: 15, length: 3, token: Token::Word("Áª≠".to_string()) },
                PositionalToken { offset: 18, length: 3, token: Token::Word("Ââß".to_string()) },
                PositionalToken { offset: 21, length: 3, token: Token::Punctuation("„Ää".to_string()) },
                PositionalToken { offset: 24, length: 3, token: Token::Word("Ë∂Ö".to_string()) },
                PositionalToken { offset: 27, length: 3, token: Token::Word("‰∫∫".to_string()) },
                PositionalToken { offset: 30, length: 3, token: Token::Word("Ââç".to_string()) },
                PositionalToken { offset: 33, length: 3, token: Token::Word("‰º†".to_string()) },
                PositionalToken { offset: 36, length: 3, token: Token::Punctuation("„Äã".to_string()) },
                PositionalToken { offset: 39, length: 3, token: Token::Word("ÁöÑ".to_string()) },
                PositionalToken { offset: 42, length: 3, token: Token::Word("Á¨¨".to_string()) },
                PositionalToken { offset: 45, length: 3, token: Token::Word("‰∏Ä".to_string()) },
                PositionalToken { offset: 48, length: 3, token: Token::Word("ÈõÜ".to_string()) },
                PositionalToken { offset: 51, length: 3, token: Token::Punctuation("„Ää".to_string()) },
                PositionalToken { offset: 54, length: 3, token: Token::Word("ËØï".to_string()) },
                PositionalToken { offset: 57, length: 3, token: Token::Word("Êí≠".to_string()) },
                PositionalToken { offset: 60, length: 3, token: Token::Word("ÈõÜ".to_string()) },
                PositionalToken { offset: 63, length: 3, token: Token::Punctuation("„Äã".to_string()) },
                PositionalToken { offset: 66, length: 3, token: Token::Word("‰∫é".to_string()) },
                PositionalToken { offset: 69, length: 4, token: Token::Number(Number::Integer(2001)) },
                PositionalToken { offset: 73, length: 3, token: Token::Word("Âπ¥".to_string()) },
                PositionalToken { offset: 76, length: 2, token: Token::Number(Number::Integer(10)) },
                PositionalToken { offset: 78, length: 3, token: Token::Word("Êúà".to_string()) },
                PositionalToken { offset: 81, length: 2, token: Token::Number(Number::Integer(16)) },
                PositionalToken { offset: 83, length: 3, token: Token::Word("Êó•".to_string()) },
                PositionalToken { offset: 86, length: 3, token: Token::Word("Âú®".to_string()) },
                PositionalToken { offset: 89, length: 3, token: Token::Word("Èõª".to_string()) },
                PositionalToken { offset: 92, length: 3, token: Token::Word("Ë¶ñ".to_string()) },
                PositionalToken { offset: 95, length: 3, token: Token::Word("Á∂≤".to_string()) },
                PositionalToken { offset: 98, length: 3, token: Token::Word("È¶ñ".to_string()) },
                PositionalToken { offset: 101, length: 3, token: Token::Word("Êí≠".to_string()) },
                PositionalToken { offset: 104, length: 3, token: Token::Punctuation("Ôºå".to_string()) },
                PositionalToken { offset: 107, length: 3, token: Token::Word("Ââß".to_string()) },
                PositionalToken { offset: 110, length: 3, token: Token::Word("ÈõÜ".to_string()) },
                PositionalToken { offset: 113, length: 3, token: Token::Word("‰∏ª".to_string()) },
                PositionalToken { offset: 116, length: 3, token: Token::Word("Âàõ".to_string()) },
                PositionalToken { offset: 119, length: 3, token: Token::Word("‰∫∫".to_string()) },
                PositionalToken { offset: 122, length: 3, token: Token::Word("Èòø".to_string()) },
                PositionalToken { offset: 125, length: 3, token: Token::Word("Â∞î".to_string()) },
                PositionalToken { offset: 128, length: 3, token: Token::Word("Âºó".to_string()) },
                PositionalToken { offset: 131, length: 3, token: Token::Word("Èõ∑".to_string()) },
                PositionalToken { offset: 134, length: 3, token: Token::Word("Âæ∑".to_string()) },
                PositionalToken { offset: 137, length: 2, token: Token::Punctuation("¬∑".to_string()) },
                PositionalToken { offset: 139, length: 3, token: Token::Word("È´ò".to_string()) },
                PositionalToken { offset: 142, length: 3, token: Token::Word("Â§´".to_string()) },
                PositionalToken { offset: 145, length: 3, token: Token::Word("Âíå".to_string()) },
                PositionalToken { offset: 148, length: 3, token: Token::Word("Ëøà".to_string()) },
                PositionalToken { offset: 151, length: 3, token: Token::Word("Â∞î".to_string()) },
                PositionalToken { offset: 154, length: 3, token: Token::Word("ÊñØ".to_string()) },
                PositionalToken { offset: 157, length: 2, token: Token::Punctuation("¬∑".to_string()) },
                PositionalToken { offset: 159, length: 3, token: Token::Word("Á±≥".to_string()) },
                PositionalToken { offset: 162, length: 3, token: Token::Word("Âãí".to_string()) },
                PositionalToken { offset: 165, length: 3, token: Token::Word("Á∑®".to_string()) },
                PositionalToken { offset: 168, length: 3, token: Token::Word("Âäá".to_string()) },
                PositionalToken { offset: 171, length: 3, token: Token::Punctuation("Ôºå".to_string()) },
                PositionalToken { offset: 174, length: 3, token: Token::Word("Â§ß".to_string()) },
                PositionalToken { offset: 177, length: 3, token: Token::Word("Âç´".to_string()) },
                PositionalToken { offset: 180, length: 2, token: Token::Punctuation("¬∑".to_string()) },
                PositionalToken { offset: 182, length: 3, token: Token::Word("Âä™".to_string()) },
                PositionalToken { offset: 185, length: 3, token: Token::Word("Áâπ".to_string()) },
                PositionalToken { offset: 188, length: 3, token: Token::Word("Â∞î".to_string()) },
                PositionalToken { offset: 191, length: 3, token: Token::Word("Êâß".to_string()) },
                PositionalToken { offset: 194, length: 3, token: Token::Word("ÂØº".to_string()) },
                PositionalToken { offset: 197, length: 3, token: Token::Punctuation("„ÄÇ".to_string()) },
                PositionalToken { offset: 200, length: 3, token: Token::Word("Ëøô".to_string()) },
                PositionalToken { offset: 203, length: 3, token: Token::Word("‰∏Ä".to_string()) },
                PositionalToken { offset: 206, length: 3, token: Token::Word("ËØï".to_string()) },
                PositionalToken { offset: 209, length: 3, token: Token::Word("Êí≠".to_string()) },
                PositionalToken { offset: 212, length: 3, token: Token::Word("È¶ñ".to_string()) },
                PositionalToken { offset: 215, length: 3, token: Token::Word("Ê¨°".to_string()) },
                PositionalToken { offset: 218, length: 3, token: Token::Word("Âêë".to_string()) },
                PositionalToken { offset: 221, length: 3, token: Token::Word("ËßÇ".to_string()) },
                PositionalToken { offset: 224, length: 3, token: Token::Word("‰ºó".to_string()) },
                PositionalToken { offset: 227, length: 3, token: Token::Word("Âºï".to_string()) },
                PositionalToken { offset: 230, length: 3, token: Token::Word("Ëçê".to_string()) },
                PositionalToken { offset: 233, length: 3, token: Token::Word("‰∫Ü".to_string()) },
                PositionalToken { offset: 236, length: 3, token: Token::Word("ÂÖã".to_string()) },
                PositionalToken { offset: 239, length: 3, token: Token::Word("Êãâ".to_string()) },
                PositionalToken { offset: 242, length: 3, token: Token::Word("ÂÖã".to_string()) },
                PositionalToken { offset: 245, length: 2, token: Token::Punctuation("¬∑".to_string()) },
                PositionalToken { offset: 247, length: 3, token: Token::Word("ËÇØ".to_string()) },
                PositionalToken { offset: 250, length: 3, token: Token::Word("Áâπ".to_string()) },
                PositionalToken { offset: 253, length: 3, token: Token::Word("‰∏Ä".to_string()) },
                PositionalToken { offset: 256, length: 3, token: Token::Word("Ëßí".to_string()) },
                PositionalToken { offset: 259, length: 3, token: Token::Punctuation("Ôºå".to_string()) },
                PositionalToken { offset: 262, length: 3, token: Token::Word("‰ªñ".to_string()) },
                PositionalToken { offset: 265, length: 3, token: Token::Word("ÊòØ".to_string()) },
                PositionalToken { offset: 268, length: 3, token: Token::Word("‰Ωç".to_string()) },
                PositionalToken { offset: 271, length: 3, token: Token::Word("Êã•".to_string()) },
                PositionalToken { offset: 274, length: 3, token: Token::Word("Êúâ".to_string()) },
                PositionalToken { offset: 277, length: 3, token: Token::Word("Ë∂Ö".to_string()) },
                ],
            Lang::Jpn => vec![
                PositionalToken { offset: 0, length: 3, token: Token::Word("ÁÜä".to_string()) },
                PositionalToken { offset: 3, length: 3, token: Token::Word("Èáé".to_string()) },
                PositionalToken { offset: 6, length: 3, token: Token::Word("‰∏â".to_string()) },
                PositionalToken { offset: 9, length: 3, token: Token::Word("Â±±".to_string()) },
                PositionalToken { offset: 12, length: 3, token: Token::Word("Êú¨".to_string()) },
                PositionalToken { offset: 15, length: 3, token: Token::Word("È°ò".to_string()) },
                PositionalToken { offset: 18, length: 3, token: Token::Word("ÊâÄ".to_string()) },
                PositionalToken { offset: 21, length: 3, token: Token::Word("„ÅØ".to_string()) },
                PositionalToken { offset: 24, length: 3, token: Token::Punctuation("„ÄÅ".to_string()) },
                PositionalToken { offset: 27, length: 2, token: Token::Number(Number::Integer(15)) },
                PositionalToken { offset: 29, length: 3, token: Token::Word("‰∏ñ".to_string()) },
                PositionalToken { offset: 32, length: 3, token: Token::Word("Á¥Ä".to_string()) },
                PositionalToken { offset: 35, length: 3, token: Token::Word("Êú´".to_string()) },
                PositionalToken { offset: 38, length: 3, token: Token::Word("‰ª•".to_string()) },
                PositionalToken { offset: 41, length: 3, token: Token::Word("Èôç".to_string()) },
                PositionalToken { offset: 44, length: 3, token: Token::Word("„Å´".to_string()) },
                PositionalToken { offset: 47, length: 3, token: Token::Word("„Åä".to_string()) },
                PositionalToken { offset: 50, length: 3, token: Token::Word("„Åë".to_string()) },
                PositionalToken { offset: 53, length: 3, token: Token::Word("„Çã".to_string()) },
                PositionalToken { offset: 56, length: 3, token: Token::Word("ÁÜä".to_string()) },
                PositionalToken { offset: 59, length: 3, token: Token::Word("Èáé".to_string()) },
                PositionalToken { offset: 62, length: 3, token: Token::Word("‰∏â".to_string()) },
                PositionalToken { offset: 65, length: 3, token: Token::Word("Â±±".to_string()) },
                PositionalToken { offset: 68, length: 3, token: Token::Punctuation("Ôºà".to_string()) },
                PositionalToken { offset: 71, length: 3, token: Token::Word("ÁÜä".to_string()) },
                PositionalToken { offset: 74, length: 3, token: Token::Word("Èáé".to_string()) },
                PositionalToken { offset: 77, length: 3, token: Token::Word("Êú¨".to_string()) },
                PositionalToken { offset: 80, length: 3, token: Token::Word("ÂÆÆ".to_string()) },
                PositionalToken { offset: 83, length: 3, token: Token::Punctuation("„ÄÅ".to_string()) },
                PositionalToken { offset: 86, length: 3, token: Token::Word("ÁÜä".to_string()) },
                PositionalToken { offset: 89, length: 3, token: Token::Word("Èáé".to_string()) },
                PositionalToken { offset: 92, length: 3, token: Token::Word("Êñ∞".to_string()) },
                PositionalToken { offset: 95, length: 3, token: Token::Word("ÂÆÆ".to_string()) },
                PositionalToken { offset: 98, length: 3, token: Token::Punctuation("„ÄÅ".to_string()) },
                PositionalToken { offset: 101, length: 3, token: Token::Word("ÁÜä".to_string()) },
                PositionalToken { offset: 104, length: 3, token: Token::Word("Èáé".to_string()) },
                PositionalToken { offset: 107, length: 3, token: Token::Word("ÈÇ£".to_string()) },
                PositionalToken { offset: 110, length: 3, token: Token::Word("Êô∫".to_string()) },
                PositionalToken { offset: 113, length: 3, token: Token::Punctuation("Ôºâ".to_string()) },
                PositionalToken { offset: 116, length: 3, token: Token::Word("„ÅÆ".to_string()) },
                PositionalToken { offset: 119, length: 3, token: Token::Word("ÈÄ†".to_string()) },
                PositionalToken { offset: 122, length: 3, token: Token::Word("Âñ∂".to_string()) },
                PositionalToken { offset: 125, length: 3, token: Token::Punctuation("„Éª".to_string()) },
                PositionalToken { offset: 128, length: 3, token: Token::Word("‰øÆ".to_string()) },
                PositionalToken { offset: 131, length: 3, token: Token::Word("ÈÄ†".to_string()) },
                PositionalToken { offset: 134, length: 3, token: Token::Word("„ÅÆ".to_string()) },
                PositionalToken { offset: 137, length: 3, token: Token::Word("„Åü".to_string()) },
                PositionalToken { offset: 140, length: 3, token: Token::Word("„ÇÅ".to_string()) },
                PositionalToken { offset: 143, length: 3, token: Token::Word("„ÅÆ".to_string()) },
                PositionalToken { offset: 146, length: 3, token: Token::Word("Âãß".to_string()) },
                PositionalToken { offset: 149, length: 3, token: Token::Word("ÈÄ≤".to_string()) },
                PositionalToken { offset: 152, length: 3, token: Token::Word("„Çí".to_string()) },
                PositionalToken { offset: 155, length: 3, token: Token::Word("ÊãÖ".to_string()) },
                PositionalToken { offset: 158, length: 3, token: Token::Word("„Å£".to_string()) },
                PositionalToken { offset: 161, length: 3, token: Token::Word("„Åü".to_string()) },
                PositionalToken { offset: 164, length: 3, token: Token::Word("ÁµÑ".to_string()) },
                PositionalToken { offset: 167, length: 3, token: Token::Word("Áπî".to_string()) },
                PositionalToken { offset: 170, length: 3, token: Token::Word("„ÅÆ".to_string()) },
                PositionalToken { offset: 173, length: 3, token: Token::Word("Á∑è".to_string()) },
                PositionalToken { offset: 176, length: 3, token: Token::Word("Áß∞".to_string()) },
                PositionalToken { offset: 179, length: 3, token: Token::Punctuation("„ÄÇ".to_string()) },
                PositionalToken { offset: 182, length: 1, token: Token::Separator(Separator::Space) },
                PositionalToken { offset: 183, length: 3, token: Token::Word("ÁÜä".to_string()) },
                PositionalToken { offset: 186, length: 3, token: Token::Word("Èáé".to_string()) },
                PositionalToken { offset: 189, length: 3, token: Token::Word("‰∏â".to_string()) },
                PositionalToken { offset: 192, length: 3, token: Token::Word("Â±±".to_string()) },
                PositionalToken { offset: 195, length: 3, token: Token::Word("„Çí".to_string()) },
                PositionalToken { offset: 198, length: 3, token: Token::Word("Âê´".to_string()) },
                PositionalToken { offset: 201, length: 3, token: Token::Word("„ÇÅ".to_string()) },
                PositionalToken { offset: 204, length: 3, token: Token::Word("„Å¶".to_string()) },
                PositionalToken { offset: 207, length: 3, token: Token::Punctuation("„ÄÅ".to_string()) },
                PositionalToken { offset: 210, length: 3, token: Token::Word("Êó•".to_string()) },
                PositionalToken { offset: 213, length: 3, token: Token::Word("Êú¨".to_string()) },
                PositionalToken { offset: 216, length: 3, token: Token::Word("„Å´".to_string()) },
                PositionalToken { offset: 219, length: 3, token: Token::Word("„Åä".to_string()) },
                PositionalToken { offset: 222, length: 3, token: Token::Word("„Åë".to_string()) },
                PositionalToken { offset: 225, length: 3, token: Token::Word("„Çã".to_string()) },
                PositionalToken { offset: 228, length: 3, token: Token::Word("Âè§".to_string()) },
                PositionalToken { offset: 231, length: 3, token: Token::Word("‰ª£".to_string()) },
                PositionalToken { offset: 234, length: 3, token: Token::Word("„Åã".to_string()) },
                PositionalToken { offset: 237, length: 3, token: Token::Word("„Çâ".to_string()) },
                PositionalToken { offset: 240, length: 3, token: Token::Word("‰∏≠".to_string()) },
                PositionalToken { offset: 243, length: 3, token: Token::Word("‰∏ñ".to_string()) },
                PositionalToken { offset: 246, length: 3, token: Token::Word("Ââç".to_string()) },
                PositionalToken { offset: 249, length: 3, token: Token::Word("Âçä".to_string()) },
                PositionalToken { offset: 252, length: 3, token: Token::Word("„Å´".to_string()) },
                PositionalToken { offset: 255, length: 3, token: Token::Word("„Åã".to_string()) },
                PositionalToken { offset: 258, length: 3, token: Token::Word("„Åë".to_string()) },
                PositionalToken { offset: 261, length: 3, token: Token::Word("„Å¶".to_string()) },
                PositionalToken { offset: 264, length: 3, token: Token::Word("„ÅÆ".to_string()) },
                PositionalToken { offset: 267, length: 3, token: Token::Word("ÂØ∫".to_string()) },
                PositionalToken { offset: 270, length: 3, token: Token::Word("Á§æ".to_string()) },
                PositionalToken { offset: 273, length: 3, token: Token::Word("„ÅÆ".to_string()) },
                PositionalToken { offset: 276, length: 3, token: Token::Word("ÈÄ†".to_string()) },
                PositionalToken { offset: 279, length: 3, token: Token::Word("Âñ∂".to_string()) },
                PositionalToken { offset: 282, length: 3, token: Token::Word("„ÅØ".to_string()) },
                PositionalToken { offset: 285, length: 3, token: Token::Punctuation("„ÄÅ".to_string()) },
                PositionalToken { offset: 288, length: 3, token: Token::Word("ÂØ∫".to_string()) },
                PositionalToken { offset: 291, length: 3, token: Token::Word("Á§æ".to_string()) },
                ],
            Lang::Kor => vec![
                PositionalToken { offset: 0, length: 21, token: Token::Word("ÌîåÎ†àÏù¥Ïä§ÌÖåÏù¥ÏÖò".to_string()) },
                PositionalToken { offset: 21, length: 1, token: Token::Separator(Separator::Space) },
                PositionalToken { offset: 22, length: 3, token: Token::Word("ÏùÄ".to_string()) },
                PositionalToken { offset: 25, length: 1, token: Token::Separator(Separator::Space) },
                PositionalToken { offset: 26, length: 6, token: Token::Word("ÏÜåÎãà".to_string()) },
                PositionalToken { offset: 32, length: 1, token: Token::Separator(Separator::Space) },
                PositionalToken { offset: 33, length: 9, token: Token::Word("Ïª¥Ìì®ÌÑ∞".to_string()) },
                PositionalToken { offset: 42, length: 1, token: Token::Separator(Separator::Space) },
                PositionalToken { offset: 43, length: 21, token: Token::Word("ÏóîÌÑ∞ÌÖåÏù∏Î®ºÌä∏Í∞Ä".to_string()) },
                PositionalToken { offset: 64, length: 1, token: Token::Separator(Separator::Space) },
                PositionalToken { offset: 65, length: 9, token: Token::Word("Í∞úÎ∞úÌïú".to_string()) },
                PositionalToken { offset: 74, length: 1, token: Token::Separator(Separator::Space) },
                PositionalToken { offset: 75, length: 3, token: Token::Word("ÏÑ∏".to_string()) },
                PositionalToken { offset: 78, length: 1, token: Token::Separator(Separator::Space) },
                PositionalToken { offset: 79, length: 6, token: Token::Word("Î≤àÏß∏".to_string()) },
                PositionalToken { offset: 85, length: 1, token: Token::Separator(Separator::Space) },
                PositionalToken { offset: 86, length: 9, token: Token::Word("Í∞ÄÏ†ïÏö©".to_string()) },
                PositionalToken { offset: 95, length: 1, token: Token::Separator(Separator::Space) },
                PositionalToken { offset: 96, length: 15, token: Token::Word("Í≤åÏûÑÍ∏∞Ïù¥Îã§".to_string()) },
                PositionalToken { offset: 111, length: 1, token: Token::Punctuation(".".to_string()) },
                PositionalToken { offset: 112, length: 1, token: Token::Separator(Separator::Space) },
                PositionalToken { offset: 113, length: 24, token: Token::Word("ÎßàÏù¥ÌÅ¨Î°úÏÜåÌîÑÌä∏Ïùò".to_string()) },
                PositionalToken { offset: 137, length: 1, token: Token::Separator(Separator::Space) },
                PositionalToken { offset: 138, length: 12, token: Token::Word("ÏóëÏä§Î∞ïÏä§".to_string()) },
                PositionalToken { offset: 150, length: 1, token: Token::Separator(Separator::Space) },
                PositionalToken { offset: 151, length: 3, token: Token::Number(Number::Integer(360)) },
                PositionalToken { offset: 154, length: 1, token: Token::Punctuation(",".to_string()) },
                PositionalToken { offset: 155, length: 1, token: Token::Separator(Separator::Space) },
                PositionalToken { offset: 156, length: 12, token: Token::Word("ÎãåÌÖêÎèÑÏùò".to_string()) },
                PositionalToken { offset: 168, length: 1, token: Token::Separator(Separator::Space) },
                PositionalToken { offset: 169, length: 6, token: Token::Word("WiiÏôÄ".to_string()) },
                PositionalToken { offset: 175, length: 1, token: Token::Separator(Separator::Space) },
                PositionalToken { offset: 176, length: 12, token: Token::Word("Í≤ΩÏüÅÌïòÍ≥†".to_string()) },
                PositionalToken { offset: 188, length: 1, token: Token::Separator(Separator::Space) },
                PositionalToken { offset: 189, length: 6, token: Token::Word("ÏûàÎã§".to_string()) },
                PositionalToken { offset: 195, length: 1, token: Token::Punctuation(".".to_string()) },
                PositionalToken { offset: 196, length: 1, token: Token::Separator(Separator::Space) },
                PositionalToken { offset: 197, length: 6, token: Token::Word("Ïù¥Ï†Ñ".to_string()) },
                PositionalToken { offset: 203, length: 1, token: Token::Separator(Separator::Space) },
                PositionalToken { offset: 204, length: 12, token: Token::Word("Ï†úÌíàÏóêÏÑú".to_string()) },
                PositionalToken { offset: 216, length: 1, token: Token::Separator(Separator::Space) },
                PositionalToken { offset: 217, length: 9, token: Token::Word("Ïò®ÎùºÏù∏".to_string()) },
                PositionalToken { offset: 226, length: 1, token: Token::Separator(Separator::Space) },
                PositionalToken { offset: 227, length: 9, token: Token::Word("ÌîåÎ†àÏù¥".to_string()) },
                PositionalToken { offset: 236, length: 1, token: Token::Separator(Separator::Space) },
                PositionalToken { offset: 237, length: 3, token: Token::Word("Í∏∞".to_string()) },
                ],
            Lang::Ara => vec![
                PositionalToken { offset: 0, length: 14, token: Token::Word("ŸÑÿ¥⁄©ÿ±⁄©ÿ¥€å".to_string()) },
                PositionalToken { offset: 14, length: 3, token: Token::UnicodeFormater(Formater::Char('\u{200c}')) },
                PositionalToken { offset: 17, length: 6, token: Token::Word("Ÿáÿß€å".to_string()) },
                PositionalToken { offset: 23, length: 1, token: Token::Separator(Separator::Space) },
                PositionalToken { offset: 24, length: 6, token: Token::Word("ÿ±Ÿàÿ≥".to_string()) },
                PositionalToken { offset: 30, length: 3, token: Token::UnicodeFormater(Formater::Char('\u{200c}')) },
                PositionalToken { offset: 33, length: 6, token: Token::Word("Ÿáÿß€å".to_string()) },
                PositionalToken { offset: 39, length: 1, token: Token::Separator(Separator::Space) },
                PositionalToken { offset: 40, length: 12, token: Token::Word("Ÿàÿßÿ±ŸÜ⁄Ø€å".to_string()) },
                PositionalToken { offset: 52, length: 1, token: Token::Separator(Separator::Space) },
                PositionalToken { offset: 53, length: 4, token: Token::Word("ÿ®Ÿá".to_string()) },
                PositionalToken { offset: 57, length: 1, token: Token::Separator(Separator::Space) },
                PositionalToken { offset: 58, length: 10, token: Token::Word("ÿØÿ±€åÿß€å".to_string()) },
                PositionalToken { offset: 68, length: 1, token: Token::Separator(Separator::Space) },
                PositionalToken { offset: 69, length: 6, token: Token::Word("ÿÆÿ≤ÿ±".to_string()) },
                PositionalToken { offset: 75, length: 1, token: Token::Separator(Separator::Space) },
                PositionalToken { offset: 76, length: 12, token: Token::Word("ŸÖÿ¨ŸÖŸàÿπŸá".to_string()) },
                PositionalToken { offset: 88, length: 3, token: Token::UnicodeFormater(Formater::Char('\u{200c}')) },
                PositionalToken { offset: 91, length: 4, token: Token::Word("ÿß€å".to_string()) },
                PositionalToken { offset: 95, length: 1, token: Token::Separator(Separator::Space) },
                PositionalToken { offset: 96, length: 4, token: Token::Word("ÿßÿ≤".to_string()) },
                PositionalToken { offset: 100, length: 1, token: Token::Separator(Separator::Space) },
                PositionalToken { offset: 101, length: 10, token: Token::Word("ÿ≠ŸÖŸÑÿßÿ™".to_string()) },
                PositionalToken { offset: 111, length: 1, token: Token::Separator(Separator::Space) },
                PositionalToken { offset: 112, length: 10, token: Token::Word("ŸÜÿ∏ÿßŸÖ€å".to_string()) },
                PositionalToken { offset: 122, length: 1, token: Token::Separator(Separator::Space) },
                PositionalToken { offset: 123, length: 4, token: Token::Word("ÿØÿ±".to_string()) },
                PositionalToken { offset: 127, length: 1, token: Token::Separator(Separator::Space) },
                PositionalToken { offset: 128, length: 6, token: Token::Word("ÿ®€åŸÜ".to_string()) },
                PositionalToken { offset: 134, length: 1, token: Token::Separator(Separator::Space) },
                PositionalToken { offset: 135, length: 6, token: Token::Word("ÿ≥ÿßŸÑ".to_string()) },
                PositionalToken { offset: 141, length: 3, token: Token::UnicodeFormater(Formater::Char('\u{200c}')) },
                PositionalToken { offset: 144, length: 6, token: Token::Word("Ÿáÿß€å".to_string()) },
                PositionalToken { offset: 150, length: 1, token: Token::Separator(Separator::Space) },
                PositionalToken { offset: 151, length: 6, token: Token::StrangeWord("€∏€∂€¥".to_string()) },
                PositionalToken { offset: 157, length: 1, token: Token::Separator(Separator::Space) },
                PositionalToken { offset: 158, length: 4, token: Token::Word("ÿ™ÿß".to_string()) },
                PositionalToken { offset: 162, length: 1, token: Token::Separator(Separator::Space) },
                PositionalToken { offset: 163, length: 8, token: Token::StrangeWord("€±€∞€¥€±".to_string()) },
                PositionalToken { offset: 171, length: 1, token: Token::Separator(Separator::Space) },
                PositionalToken { offset: 172, length: 12, token: Token::Word("ŸÖ€åŸÑÿßÿØ€å".to_string()) },
                PositionalToken { offset: 184, length: 1, token: Token::Separator(Separator::Space) },
                PositionalToken { offset: 185, length: 2, token: Token::Word("ÿ®".to_string()) },
                ],
            Lang::Ell => vec![
                PositionalToken { offset: 0, length: 4, token: Token::Word("Œ§Œø".to_string()) },
                PositionalToken { offset: 4, length: 1, token: Token::Separator(Separator::Space) },
                PositionalToken { offset: 5, length: 18, token: Token::Word("Œ†œÅœåŒ≥œÅŒ±ŒºŒºŒ±".to_string()) },
                PositionalToken { offset: 23, length: 1, token: Token::Separator(Separator::Space) },
                PositionalToken { offset: 24, length: 22, token: Token::Word("œÖŒªŒøœÄŒøŒπŒµŒØœÑŒ±Œπ".to_string()) },
                PositionalToken { offset: 46, length: 1, token: Token::Separator(Separator::Space) },
                PositionalToken { offset: 47, length: 4, token: Token::Word("ŒµŒæ".to_string()) },
                PositionalToken { offset: 51, length: 1, token: Token::Separator(Separator::Space) },
                PositionalToken { offset: 52, length: 18, token: Token::Word("ŒøŒªŒøŒ∫ŒªŒÆœÅŒøœÖ".to_string()) },
                PositionalToken { offset: 70, length: 1, token: Token::Separator(Separator::Space) },
                PositionalToken { offset: 71, length: 6, token: Token::Word("Œ±œÄœå".to_string()) },
                PositionalToken { offset: 77, length: 1, token: Token::Separator(Separator::Space) },
                PositionalToken { offset: 78, length: 16, token: Token::Word("Œ±œÄœåœÉœÑŒ±œÉŒ∑".to_string()) },
                PositionalToken { offset: 94, length: 1, token: Token::Separator(Separator::Space) },
                PositionalToken { offset: 95, length: 6, token: Token::Word("Œ∫Œ±Œπ".to_string()) },
                PositionalToken { offset: 101, length: 1, token: Token::Separator(Separator::Space) },
                PositionalToken { offset: 102, length: 12, token: Token::Word("ŒºœÄŒøœÅŒµŒØ".to_string()) },
                PositionalToken { offset: 114, length: 1, token: Token::Separator(Separator::Space) },
                PositionalToken { offset: 115, length: 4, token: Token::Word("ŒΩŒ±".to_string()) },
                PositionalToken { offset: 119, length: 1, token: Token::Separator(Separator::Space) },
                PositionalToken { offset: 120, length: 20, token: Token::Word("œÉœÖŒºŒºŒµœÑŒ≠œáŒµŒπ".to_string()) },
                PositionalToken { offset: 140, length: 1, token: Token::Separator(Separator::Space) },
                PositionalToken { offset: 141, length: 8, token: Token::Word("Œ∫Œ¨Œ∏Œµ".to_string()) },
                PositionalToken { offset: 149, length: 1, token: Token::Separator(Separator::Space) },
                PositionalToken { offset: 150, length: 24, token: Token::Word("ŒµŒºœÄŒªŒµŒ∫œåŒºŒµŒΩŒøœÇ".to_string()) },
                PositionalToken { offset: 174, length: 1, token: Token::Separator(Separator::Space) },
                PositionalToken { offset: 175, length: 6, token: Token::Word("œÉœÑŒ∑".to_string()) },
                PositionalToken { offset: 181, length: 1, token: Token::Separator(Separator::Space) },
                PositionalToken { offset: 182, length: 2, token: Token::Word("ŒÆ".to_string()) },
                PositionalToken { offset: 184, length: 1, token: Token::Punctuation("/".to_string()) },
                ],
        };
        (text,tokens)
    }
}
 
